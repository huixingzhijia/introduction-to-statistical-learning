---
title: "Homework R Markdown"
author: "Wenhui Zeng"
date: "February 7, 2017"
output: pdf_document
---





##2
Assuming that $f_k(x)$ is normal, the probability that an observation $x$ is in class $k$ is given by
$$f_k(x)=\frac{\pi_k\frac{1}{\sqrt(2\pi\sigma)}e^{-\frac{1}{2\sigma^2}{(x-\mu_k)^2}}}{\sum_{l=1}^{k}\pi_l\frac{1}{\sqrt(2\pi\sigma)}e^{-\frac{1}{2\sigma^2}{(x-\mu_l)^2}}}$$
while the discriminant function was 
$$\delta_k(x)=x\frac{\mu_k}{\sigma^2}-\frac{\mu_k}{2\sigma^2}+\log(\pi_k)$$
The question also means maxmizing $f_k(x)$ is equivalent to maxmizing $\delta_k(x)$
Let $x$ remain fixed and observe that we are maxmizing over the parameter k. suppose that $\delta_k(x) \geq \delta_i(x)$. We will prove that $f_k(x) \geq f_i(x)$

$$x\frac{\mu_k}{\sigma^2}-\frac{\mu_k}{2\sigma^2}+\log(\pi_k) \geq x\frac{\mu_i}{\sigma^2}-\frac{\mu_i}{2\sigma^2}+\log(\pi_i)$$
Exponentiation is a monotonically increasing function, so the following inequality holds
$$\pi_ke^{(x\frac{\mu_k}{\sigma^2}-\frac{\mu_k}{2\sigma^2})} \geq \pi_ie^{(x\frac{\mu_i}{\sigma^2}-\frac{\mu_i}{2\sigma^2})}$$
Multiple this by $C$ 
$$C=\frac{\frac{1}{\sqrt(2\pi\sigma)}e^{-\frac{1}{2\sigma^2}{x^2}}}{\sum_{l=1}^{k}\frac{1}{\sqrt(2\pi\sigma)}e^{-\frac{1}{2\sigma^2}{(x-\mu_l)^2}}}$$
Then it gives the following equation

$$\frac{\pi_k\frac{1}{\sqrt(2\pi\sigma)}e^{(x\frac{\mu_k}{\sigma^2}-\frac{\mu_k}{2\sigma^2}
-\frac{1}{2\sigma^2}x^2)}}
{\sum_{l=1}^{k}\pi_l\frac{1}{\sqrt(2\pi\sigma)}e^{-\frac{1}{2\sigma^2}{(x-\mu_l)^2}}} \geq \frac{\pi_i\frac{1}{\sqrt(2\pi\sigma)}e^{(x\frac{\mu_i}{\sigma^2}-\frac{\mu_i}{2\sigma^2}
-\frac{1}{2\sigma^2}x^2)}}
{\sum_{l=1}^{k}\pi_l\frac{1}{\sqrt(2\pi\sigma)}e^{-\frac{1}{2\sigma^2}{(x-\mu_l)^2}}} $$






which equals to 

$$\frac{\pi_k\frac{1}{\sqrt(2\pi\sigma)}e^{-\frac{1}{2\sigma^2}{(x-\mu_k)^2}}}
{\sum_{l=1}^{k}\pi_l\frac{1}{\sqrt(2\pi\sigma)}e^{\frac{-1}{2\sigma^2}{(x-\mu_l)^2}}} \geq \frac{\pi_i\frac{1}{\sqrt(2\pi\sigma)}e^{-\frac{1}{2\sigma^2}{(x-\mu_i)^2}}}
{\sum_{l=1}^{k}\pi_l\frac{1}{\sqrt(2\pi\sigma)}e^{-\frac{1}{2\sigma^2}{(x-\mu_l)^2}}} $$

So maxmizing $\delta_k(x)$ is equivalent to maximizing $p_k(x)$


##3
We are not assuming that the variance is not equal
$$f_k(x)=\frac{\pi_k\frac{1}{\sqrt(2\pi\sigma_k)}e^{-\frac{1}{2\sigma_k^2}{(x-\mu_k)^2}}}{\sum_{l=1}^{k}\pi_l\frac{1}{\sqrt(2\pi\sigma_l)}e^{-\frac{1}{2\sigma_l^2}{(x-\mu_l)^2}}}$$

$$\log(f_k(x))=\frac{\log\pi_k+\log\frac{1}{\sqrt{2\pi\sigma_k}}-\frac{1}{2\sigma_k^2}(x-\mu_k)^2}{
\log{(\sum_{l=1}^{k}\pi_l\frac{1}{\sqrt(2\pi\sigma_l)}e^{-\frac{1}{2\sigma_l^2}{(x-\mu_l)^2}}})}$$







$$\log(f_k(x))\log{(\sum_{l=1}^{k}\pi_l\frac{1}{\sqrt(2\pi\sigma_l)}e^{-\frac{1}{2\sigma_l^2}{(x-\mu_l)^2}}})=\delta_p(x)=\log\pi_k+\log\frac{1}{\sqrt{2\pi\sigma_k}}-\frac{1}{2\sigma_k^2}(x-\mu_k)^2$$


As we can see the $\delta_p(x)$ was  quadratic function of x.

##5
### a
If the Bayes decision boundary is linear, we expect QDA to perform better on the training set because it's higher flexibility will yield a closer fit one the test set, we expect LDA to perform better than QDA because QDA could overfit the linerity of the Bayes decision boundary

###b
If the Bayes decision boundary is non-linear. QDA perform better both on the training and test sets

###c
We expect the test prediction accuracy of QDA relative to LDA to improve in general, as the sample size n increases because a more 
flexible method will yield a better fit as more samples can be fit and variance is offset by the larger sample sizes

###d
False, with fewer sample points, the variance from using a more flexible method, such as QDA, would lead to overfit, yielding a higher test rate than LDA


## 7 

$$f_k(x)=\frac{\pi_k\frac{1}{\sqrt(2\pi\sigma)}e^{-\frac{1}{2\sigma^2}{(x-\mu_k)^2}}}{\sum_{l=1}^{k}\pi_l\frac{1}{\sqrt(2\pi\sigma)}e^{-\frac{1}{2\sigma^2}{(x-\mu_l)^2}}}=
\frac{\pi_1e^{-\frac{1}{2\sigma^2}{(x-\mu_1)^2}}}{\pi_1e^{-\frac{1}{2\sigma^2}{(x-\mu_1)^2}}+
\pi_0e^{-\frac{1}{2\sigma^2}{(x-\mu_0)^2}}}$$

$$=\frac{0.8\exp(-\frac{1}{2*36}(4-10)^2)}{0.8\exp(-\frac{1}{2*36}(4-10)^2)+0.2\exp(-\frac{1}{2*36}4^2)}=75.2\%$$


##10
###e
```{r,echo=F,warning=F}
library(ISLR)
library(MASS)
library(xtable)
library(knitr)
library(class)
train<-(Weekly$Year<2009)
weekly.0910<-Weekly[!train,]
lda.train<-lda(Direction~Lag2,data=Weekly,subset=train)
#predict the 0910 datasets using the prediction fit from glm.fit
lda.prob2<-predict(lda.train,weekly.0910)
#lda.prob2 has three variables, like class, posterior and x, the class variable gives the classification 
#of up or down for the direction
lda.class<-lda.prob2$class
table<-table(lda.class,weekly.0910$Direction)
e<-mean(lda.class==weekly.0910$Direction)
```

The confusion table is below
```{r, echo=FALSE, results='asis',warning=F}
print(xtable(table), type = "latex", comment = FALSE)
```

The overall fraction of correct predictions for the held out data is  `r e` 

###f
```{r,echo=F,warning=F}
weekly.train<-Weekly[train,]
qda.train<-qda(Direction~Lag2,data=Weekly,subset=train)
qda.class<-predict(qda.train,weekly.0910)$class
table<-table(qda.class,weekly.0910$Direction)
e<-mean(qda.class==weekly.0910$Direction)
```

The confusion table is below
```{r, echo=FALSE, results='asis',warning=F}
print(xtable(table), type = "latex", comment = FALSE)
```
The overall fraction of correct predictions for the held out data is  `r e` 

###g
```{r,echo=F,warning=F}
library(class)
weekly.train <-with(Weekly,as.matrix(Lag2[train]))
weekly.test<-with(Weekly,as.matrix(Lag2[!train]))
train.Direction <- with(Weekly,Direction[train])
set.seed(1)
knn.pred<-knn(weekly.train,weekly.test,train.Direction,k=1)
Direction.0910 <- with(Weekly,Direction[!train])
table<-table(knn.pred,Direction.0910)
e<-mean(knn.pred==Direction.0910)
```

The confusion table is below
```{r, echo=FALSE, results='asis',warning=F}
print(xtable(table), type = "latex", comment = FALSE)
```
The overall fraction of correct predictions for the held out data is  `r e` 


###h 
Logistic regression and LDA methods provide similar test error rates.


###i 
```{r,echo=F,warning=F}
glm.fit4<-glm(Direction~Lag1:Lag2,data=Weekly,family=binomial,subset=train)
glm.prob<-predict(glm.fit4,weekly.0910,type="response")
glm.pred<-rep("Down",length(glm.prob))
#transform
glm.pred[glm.prob>0.5]="Up"
table<-table(glm.pred,weekly.0910$Direction)
e<-mean(glm.pred==weekly.0910$Direction)
```

The overall fraction of correct predictions for the held out data is  `r e` 
Using logistic regression with interaction term,the confusion table is below
```{r, echo=FALSE, results='asis',warning=F}
print(xtable(table), type = "latex", comment = FALSE)
```


####i-e 
```{r,echo=F,warning=F}
lda.train<-lda(Direction~Lag1:Lag2,data=Weekly,subset=train)
#predict the 0910 datasets using the prediction fit from glm.fit
lda.prob2<-predict(lda.train,weekly.0910)
#lda.prob2 has three variables, like class, posterior and x, the class variable gives the classification 
#of up or down for the direction
lda.class<-lda.prob2$class
table<-table(lda.class,weekly.0910$Direction)
#gives us the test error rate by using 1-the mean value
e<-mean(lda.class==weekly.0910$Direction)
```
The overall fraction of correct predictions for the held out data is  `r e` 
Using linear Discriminant Analysis with interaction term,the confusion table is below
```{r, echo=FALSE, results='asis',warning=F}
print(xtable(table), type = "latex", comment = FALSE)
```

####i-f
```{r,echo=F,warning=F}
qda.train<-qda(Direction~sqrt(abs(Lag2))+Lag2,data=Weekly,subset=train)
qda.class<-predict(qda.train,weekly.0910)$class
table<-table(qda.class,weekly.0910$Direction)
e<-mean(qda.class==weekly.0910$Direction)
```
The overall fraction of correct predictions for the held out data is  `r e` 
Using QDA  with interaction term,the confusion table is below
```{r, echo=FALSE, results='asis',warning=F}
print(xtable(table), type = "latex", comment = FALSE)
```


####i-g
```{r,echo=F,warning=F}
knn.pred=knn(weekly.train,weekly.test,train.Direction,k=10)
table<-table(knn.pred,Direction.0910)
e<-mean(knn.pred==Direction.0910)
```
The overall fraction of correct predictions for the held out data is  `r e` 
Using k-nearest neighbor with k=10,the confusion table is below

```{r, echo=FALSE, results='asis',warning=F}
print(xtable(table), type = "latex", comment = FALSE)
```

####i-g
```{r,echo=F,warning=F}
knn.pred<-knn(weekly.train,weekly.test,train.Direction,k=100)
t<-table(knn.pred,Direction.0910)
e<-mean(knn.pred==Direction.0910)
```
The overall fraction of correct predictions for the held out data is  `r e` 
Using k-nearest neighbor with k=100,the confusion table is below

```{r, echo=FALSE, results='asis',warning=F}
print(xtable(t), type = "latex", comment = FALSE)
```

The original LDA and logistic regression have better performance in terms of test error rate.


##13

```{r,echo=F,warning=F}
library(MASS)
crim01<-rep(0,length(Boston$crim))
crim01[Boston$crim>median(Boston$crim)]=1
Boston<-data.frame(crim01,Boston)
#making 50% training dataset and 50% test dataset
train<-1:(dim(Boston)[1]/2)
test<-(dim(Boston)[1]/2+1):dim(Boston)[1]
Boston.train<-Boston[train,]
Boston.test<-Boston[test,]
```

####a logistic regression
```{r,echo=F,warning=F}
glm.Boston<-glm(crim01~.-crim,data=Boston.train,family = binomial)
glm.pred<-predict(glm.Boston,Boston.test,type="response")
glm.prob<-rep(0,length(glm.pred))
glm.prob[glm.pred>0.5]=1
crim01.test<-crim01[test]
t<-table(glm.prob,crim01.test)
c<-mean(glm.prob==crim01.test)
#test error is about 18%
a<-mean(glm.prob!=crim01.test)
```
If we use all the variable but crim, we will get a model which test error is `r a`


```{r,echo=F,warning=F}
glm.Boston<-glm(crim01~.-crim-ptratio-rm-medv-zn,data=Boston.train,family = binomial)
glm.pred<-predict(glm.Boston,Boston.test,type="response")
glm.prob<-rep(0,length(glm.pred))
glm.prob[glm.pred>0.5]=1
crim01.test<-crim01[test]
t<-table(glm.prob,crim01.test)
c<-mean(glm.prob==crim01.test)
#test error
a<-mean(glm.prob!=crim01.test)
```
we fit a more robust model, using the variable exclude crim, patratio,rm,medv,zn, then the test error is about `r a`

Linear Discriminant regression 

```{r,echo=F,warning=F}
#LDA
lda.Boston<-lda(crim01~.-crim,data=Boston.train)
lda.pred<-predict(lda.Boston,Boston.test)         
lda.class<-lda.pred$class
t<-table(lda.class,Boston.test$crim01)
#gives us the test error rate by using 1-the mean value
c<-mean(lda.class==Boston.test$crim01)
a<-mean(lda.class!=Boston.test$crim01)
#test error 0.1343874
```
If we use all the variable but crim, we will get a model which test error is `r a`


```{r,echo=F,warning=F}
lda.Boston=lda(crim01~indus+chas+nox+age+dis+rad,data=Boston.train)
lda.pred=predict(lda.Boston,Boston.test)         
lda.class=lda.pred$class
t<-table(lda.class,Boston.test$crim01)
#gives us the test error rate by using 1-the mean value
c<-mean(lda.class==Boston.test$crim01)
a<-mean(lda.class!=Boston.test$crim01)
#test error  0.1027668
```
we fit a more robust model, using the variable exclude crim, patratio,rm,medv,zn, then the test error is about `r a`

QDA
```{r,echo=F,warning=F}
qda.train<-qda(crim01~.-crim,data=Boston.train)
qda.class<-predict(qda.train,Boston.test)$class
t<-table(qda.class,Boston.test$crim01)
c<-mean(qda.class==Boston.test$crim01)
#test error is
a<-mean(qda.class!=Boston.test$crim01)#0.6521739
```
If we use all the variable but crim, we will get a model which test error is `r a`


KNN 
perform KNN on the training data
```{r,echo=F,waring=F}
library(class)
train<-1:(dim(Boston)[1]/2)
test<-(dim(Boston)[1]/2+1):dim(Boston)[1]
knn.train.Boston<-with(Boston,cbind(zn,indus,chas,nox,rm,age,dis,rad,ptratio,black,lstat,medv)[train,])
knn.test.Boston<-with(Boston,cbind(zn,indus,chas,nox,rm,age,dis,rad,ptratio,black,lstat,medv)[test,])
train.crim01<-crim01[train]
#k=1
set.seed(1)
knn.pred=knn(knn.train.Boston,knn.test.Boston,train.crim01,k=1)
t<-table(knn.pred,Boston.test$crim01)
a<-mean(knn.pred!=Boston.test$crim01)#0.3596838
```
With k=1 the test error is `r a`
```{r,echo=F,warning=F}
#k=10
set.seed(1)
knn.pred=knn(knn.train.Boston,knn.test.Boston,train.crim01,k=10)
t<-table(knn.pred,Boston.test$crim01)
a<-mean(knn.pred!=Boston.test$crim01)#0.2134387

```
With k=10 the test error is `r a`

```{r,echo=F,warning=F}
#k=100
set.seed(1)
knn.pred=knn(knn.train.Boston,knn.test.Boston,train.crim01,k=100)
t<-table(knn.pred,Boston.test$crim01)
a<-mean(knn.pred!=Boston.test$crim01)#the test error rate 0.3517787

```
With k=100 the test error is `r a`








