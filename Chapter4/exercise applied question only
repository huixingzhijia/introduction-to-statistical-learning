
#
library(ISLR)
names(Weekly)
dim(Weekly)
summary(Weekly)

#a
pairs(Weekly)
cor(Weekly[, -9])

#b
#Year and Volume appear to have a relationship. No other patterns are discernible.
glm.fit<-glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Weekly,family=binomial)
summary(glm.fit)
#the lag2 is statistically significant different

#c
#compute the confusion matrix and overall fraction of correct predictions.
glm.prob<-predict(glm.fit,Weekly,type="response")
glm.pred<-rep("Down",length(glm.prob))
#transform
glm.pred[glm.prob>0.5]="Up"
glm.pred
table(glm.pred,Weekly$Direction)

#d now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor
#compute the confusion matrix and the overall fraction of correct predictions for the held out data(data from 2009 to 2010)
train<-with(Weekly,Year<2009)
weekly.0910<-Weekly[!train,]
glm.train<-glm(Direction~Lag2,data=Weekly,family=binomial,subset=train)
#predict the 0910 datasets using the prediction fit from glm.fit
glm.prob1<-predict(glm.fit,weekly.0910,type="response")
glm.pred1<-rep("Down",length(glm.prob1))
glm.pred1[glm.prob1>0.5]="Up"
table(glm.pred1,weekly.0910$Direction)
mean(glm.pred1==weekly.0910$Direction)

#e repeat d using LDA
library(MASS)
weekly.0910<-Weekly[!train,]
lda.train<-lda(Direction~Lag2,data=Weekly,subset=train)
#predict the 0910 datasets using the prediction fit from glm.fit
lda.prob2<-predict(lda.train,weekly.0910)
#lda.prob2 has three variables, like class, posterior and x, the class variable gives the classification 
#of up or down for the direction
names(lda.prob2)

lda.class<-lda.prob2$class
table(lda.class,weekly.0910$Direction)
mean(lda.class==weekly.0910$Direction)

#f repeat using QDA
library(MASS)
with(Weekly,train<-(Year<2009))
weekly.0910<-Weekly[!train,]
weekly.train<-Weekly[train,]
qda.train<-qda(Direction~Lag2,data=Weekly,subset=train)
qda.class<-predict(qda.train,weekly.0910)$class
table(qda.class,weekly.0910$Direction)
mean(qda.class==weekly.0910$Direction)

#g, repeat d using KNN with K=1
library(class)
weekly.train <- as.matrix(Weekly$Lag2[train])
weekly.test<-as.matrix(Weekly$Lag2[!train])
train.Direction <- Weekly$Direction[train]
set.seed(1)
knn.pred<-knn(weekly.train,weekly.test,train.Direction,k=1)
Direction.0910 <- Weekly$Direction[!train]
table(knn.pred,Direction.0910)
mean(knn.pred==Direction.0910)

#h Logistic regression and LDA methods provide similar test error rates.


#i using lag1 and lag2, with different combinations of predictors,including possible transformations and interactions
# for each of the methods, report the variables, method, and associated confusion matrix that appears to provide the best
#results on the held out data. Note that you should also experiment with values for K in the KNN classifier

weekly.0910<-Weekly[!train,]
glm.fit4<-glm(Direction~Lag1:Lag2,data=Weekly,family=binomial,subset=train)
summary(glm.fit4)
glm.prob<-predict(glm.fit4,weekly.0910,type="response")
glm.pred<-rep("Down",length(glm.prob))
#transform
glm.pred[glm.prob>0.5]="Up"
table(glm.pred,weekly.0910$Direction)
mean(glm.pred==weekly.0910$Direction)

#e repeat d using LDA
lda.train<-lda(Direction~Lag1:Lag2,data=Weekly,subset=train)
#predict the 0910 datasets using the prediction fit from glm.fit
lda.prob2<-predict(lda.train,weekly.0910)
#lda.prob2 has three variables, like class, posterior and x, the class variable gives the classification 
#of up or down for the direction
lda.class<-lda.prob2$class
table(lda.class,weekly.0910$Direction)
#gives us the test error rate by using 1-the mean value
mean(lda.class==weekly.0910$Direction)


#f repeat using QDA
qda.train<-qda(Direction~sqrt(abs(Lag2))+Lag2,data=Weekly,subset=train)
qda.class<-predict(qda.train,weekly.0910)$class
table(qda.class,weekly.0910$Direction)
mean(qda.class==weekly.0910$Direction)

#g, repeat d using KNN with K=10 and 100,using seed(1) gives different answer
set.seed(1)
knn.pred<-knn(weekly.train,weekly.test,train.Direction,k=10)
Direction.0910 <- Weekly$Direction[!train]
table(knn.pred,Direction.0910)
mean(knn.pred==Direction.0910)

set.seed(1)
knn.pred=knn(weekly.train,weekly.test,train.Direction,k=100)
table(knn.pred,Direction.0910)
mean(knn.pred==Direction.0910)
# the original LDA and logistic regression have better performance in terms of test error rate.

#11
library(ISLR)
names(Auto)
summary(Auto)
#create a binary variable, mpg01, that contains a 1 if mpg contais a value above its median, and a 1 if mpg 
#contains a value below its median. 

mpg01<-rep(0,length(Auto$mpg))
mpg01[Auto$mpg>median(Auto$mpg)]=1
Auto<-data.frame(mpg01,Auto)

cor(Auto[,-10])

plot(cylinders, mpg01)
boxplot(cylinders, mpg01)

plot(displacement, mpg01)
boxplot(displacement, mpg01)

plot(horsepower, mpg01)
boxplot(horsepower, mpg01)

plot(year, mpg01)
boxplot(year, mpg01)

plot(acceleration, mpg01)
boxplot(acceleration, mpg01)

plot(origin, mpg01)
boxplot(origin,mpg01)

#from the boxplot, we can see that the cylinders, displacement, weight,horsepower are related
# accelaration, year and origin their R square is too small to 

#c split the data into training data and test dataset
#use even or odd year as the separation for training and testing data set

train<-(year%%2==0)
#train=(Auto$year%%2==0)
#subset the row and the column, that the rows where train is true, and all the columns
Auto.train<-Auto[train,]
Auto.test<-Auto[!train,]

#d perform LDA on the training data set in order to predict mpg01
lda.auto<-lda(mpg01~cylinders+displacement+horsepower+weight,data=Auto,subset=train)
lda.pred<-predict(lda.auto,Auto.test)         
lda.class<-lda.pred$class
t<-table(lda.class,Auto.test$mpg01)
#gives us the test error rate by using 1-the mean value
c<-mean(lda.class==Auto.test$mpg01)
#test error is 1-0.8736=0.1263
e<-mean(lda.class!=Auto.test$mpg01)

#e perform QDA on the training data in order to predict mpg01

qda.train<-qda(mpg01~cylinders+displacement+horsepower+weight,data=Auto,subset=train)
qda.class<-predict(qda.train,Auto.test)$class
table(qda.class,Auto.test$mpg01)
mean(qda.class==Auto.test$mpg01)
#test error is
mean(qda.class!=Auto.test$mpg01)#0.1318681

#f perform logistic regression 
glm.auto=glm(mpg01~cylinders+displacement+horsepower+weight,data=Auto,subset=train,family = binomial)
glm.pred=predict(glm.auto,Auto.test,type="response")
glm.prob=rep("0",length(glm.pred))
#transform
glm.prob[glm.pred>0.5]=1
table(glm.prob,Auto.test$mpg01)
mean(glm.prob==Auto.test$mpg01)
#test error
mean(glm.prob!=Auto.test$mpg01)#0.1208791

#g perform KNN on the training data, 
library(class)
knn.train=cbind(Auto$cylinders,Auto$displacement,Auto$horsepower,Auto$weight)[train,]
knn.test=cbind(Auto$cylinders,Auto$displacement,Auto$horsepower,Auto$weight)[!train,]
train.mpg01=mpg01[train]


#k=1
set.seed(1)
knn.pred=knn(knn.train,knn.test,train.mpg01,k=1)
table(knn.pred,Auto.test$mpg01)
mean(knn.pred!=Auto.test$mpg01)#0.1538462

#k=10
set.seed(1)
knn.pred=knn(knn.train,knn.test,train.mpg01,k=10)
table(knn.pred,Auto.test$mpg01)
mean(knn.pred!=Auto.test$mpg01)#0.1538462

#k=10
set.seed(1)
knn.pred=knn(knn.train,knn.test,train.mpg01,k=100)
table(knn.pred,Auto.test$mpg01)
mean(knn.pred!=Auto.test$mpg01)#0.1428571

#k=1, 15.4% test error rate. k=10, 16.5% test error rate. k=100, 14.3% test error rate. K of 100 seems 
#to perform the best. 100 nearest neighbors.

#12
#write a functio, that prints out the result of raising 2 to the 3rd power.
Power <- function(x){
  result=x^3
  return(result)
}
x=Power(2)

#b create a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out
#the value of X^a

Power2=function(x,a){
  x^a
  return
}

Power2(3,8)
Power2(10,3)
Power2(8,17)
Power2(131,3)


#create power3 

Power3=function(x,a){
  result=x^a
  return(result)
}

#e
x=1:10
#consider displaying either the x-axis, the y-axis or both on the log-scale you can do this by suing 
#log="x", log="y", log="xy"
plot(x, Power3(x, 2), log = "xy", ylab = "Log of y = x^2", xlab = "Log of x", 
     main = "Log of x^2 versus Log of x")

#f
PlotPower=function(x,a){
  plot(x,Power3(x,a))
}
PlotPower(1:10,3)

#13 using the boston data set, fit classification models in order to predict whether a given suburb has a crime
#rate above or below the median. Explore the logistic regression, LDA, KNN models using various subset of the predictors
#like Q11

#method 2
crim01<-rep(0,length(Boston$crim))
crim01[Boston$crim>median(Boston$crim)]=1
Boston<-data.frame(crim01,Boston)
cor(Boston[,-2])

cor(Boston[,-2])

#making 50% training dataset and 50% test dataset
train=1:(dim(Boston)[1]/2)
test=(dim(Boston)[1]/2+1):dim(Boston)[1]
Boston.train=Boston[train,]
Boston.test=Boston[test,]
glm.Boston=glm(crim01~.-crim -crim01,data=Boston.train,family = binomial)
glm.pred=predict(glm.Boston,Boston.test,type="response")
glm.prob=rep(0,length(glm.pred))
glm.prob[glm.pred>0.5]=1
crim01.test=crim01[test]
table(glm.prob,crim01.test)
mean(glm.prob==crim01.test)
#test error
mean(glm.prob!=crim01.test)


glm.Boston=glm(crim01~.-crim -crim01-ptratio-rm-medv-zn,data=Boston.train,family = binomial)
summary(glm.Boston)
glm.pred=predict(glm.Boston,Boston.test,type="response")
glm.prob=rep(0,length(glm.pred))
glm.prob[glm.pred>0.5]=1
crim01.test=crim01[test]
table(glm.prob,crim01.test)
mean(glm.prob==crim01.test)
#test error
mean(glm.prob!=crim01.test)
#test error 11%


#LDA
lda.Boston=lda(crim01~.-crim -crim01,data=Boston.train)
lda.pred=predict(lda.Boston,Boston.test)         
lda.class=lda.pred$class
table(lda.class,Boston.test$crim01)
#gives us the test error rate by using 1-the mean value
mean(lda.class==Boston.test$crim01)
mean(lda.class!=Boston.test$crim01)
#test error 0.1343874

lda.Boston=lda(crim01~.--crim -crim01-ptratio-rm-medv-zn,data=Boston.train)
lda.pred=predict(lda.Boston,Boston.test)         
lda.class=lda.pred$class
table(lda.class,Boston.test$crim01)
#gives us the test error rate by using 1-the mean value
mean(lda.class==Boston.test$crim01)
mean(lda.class!=Boston.test$crim01)
#test error  0.1027668


#QDA
qda.train=qda(crim01~.-crim -crim01,data=Boston.train)
qda.class=predict(qda.train,Boston.test)$class
table(qda.class,Boston.test$crim01)
mean(qda.class==Boston.test$crim01)
#test error is
mean(qda.class!=Boston.test$crim01)#0.6521739

#g perform KNN on the training data, 
library(class)
names(Boston)
knn.train.Boston=cbind(zn,indus,chas,nox,rm,age,dis,rad,ptratio,black,lstat,medv,check.names = TRU)[train,]
knn.test.Boston=cbind(zn,indus,chas,nox,rm,age,dis,rad,ptratio,black,lstat,medv)[test,]
train.crim01=crim01[train]

#k=1
set.seed(1)
knn.pred=knn(knn.train.Boston,knn.test.Boston,train.crim01,k=1)
table(knn.pred,Boston.test$crim01)
mean(knn.pred!=Boston.test$crim01)#0.3596838



#k=10
set.seed(1)
knn.pred=knn(knn.train.Boston,knn.test.Boston,train.crim01,k=10)
table(knn.pred,Boston.test$crim01)
mean(knn.pred!=Boston.test$crim01)#0.2134387

#k=100
set.seed(1)
knn.pred=knn(knn.train.Boston,knn.test.Boston,train.crim01,k=100)
table(knn.pred,Boston.test$crim01)
mean(knn.pred!=Boston.test$crim01)#the test error rate 0.3517787







