---
title: "Final Exam"
author: "Wenhui Zeng"
date: "Spring 2017"
output:
  html_document:
    css: slucor.css
    includes:
      before_body: logo.html
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
subtitle: 'HDS 5330: Predictive Modeling and Machine Learning'
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, comment = "", fig.width=8.4)
options(scipen = 200)
options(digits = 2)
```

```{r, message=F, warning=F, error=F,results="asis"}
library(foreign)
library(knitr)
llcp2014 <- read.xport(file = "LLCP2014.XPT")
```

##Introduction

The ten covariates are marital status, sex, weight, height, urbanity,sleeptime, education, income, age and number of childern. The binary outcome variable is whether have been told that the participant has a depressive disorder, including major depression or minor depression.The total 19% of the participants were experience depression and about 80.56% was not depression.So it means, if we predict all to "No", the test error will be like 19.44%. Our aim was to decrease the test error and gives a good accuracry.

*Note the following program may take a long time. especially for the support vector machine part*

```{r, message=F, warning=F, error=F,results="asis"}
LLCP2014<-llcp2014[ ,c("MARITAL","SEX","WTKG3","HTIN4","ADDEPEV2",
                       "X_AGE_G","MSCODE","CHILDREN","SLEPTIM1","X_EDUCAG","X_INCOMG")]
```

##Data Management

The outcome variable was whether the interview's has depression. In the previous study, the age, urbanity, number of children in family, martial statust, gender, weight, height, sleep time, education and income were all influence the depression. 

```{r, message=F, warning=F, error=F,results="asis"}
#X_AGE_G (Age)
LLCP2014$Age[LLCP2014$X_AGE_G==1] <-"18-24"
LLCP2014$Age[LLCP2014$X_AGE_G == 2] <- "25-34"
LLCP2014$Age[LLCP2014$X_AGE_G ==3] <- "35-44"
LLCP2014$Age[LLCP2014$X_AGE_G == 4] <- "45-54"
LLCP2014$Age[LLCP2014$X_AGE_G == 5] <- "55-64"
LLCP2014$Age[LLCP2014$X_AGE_G == 6] <- "65+"
LLCP2014$Age<- as.factor(LLCP2014$Age)

#MSCODE (Urbanity)

LLCP2014$Urbanity[LLCP2014$MSCODE == 1] <- "Center of metropolitan statistical area"
LLCP2014$Urbanity[LLCP2014$MSCODE == 2] <- "Outside metropolitan statistical area"
LLCP2014$Urbanity[LLCP2014$MSCODE == 3] <- "Suburb of metropolitan statistical area"
LLCP2014$Urbanity[LLCP2014$MSCODE == 5] <- "Non-metropolitan statistical area"
LLCP2014$Urbanity<- as.factor(LLCP2014$Urbanity)


#CHILDREN
LLCP2014$numchildren<-as.numeric(LLCP2014$CHILDREN)
LLCP2014$numchildren[LLCP2014$CHILDREN %in% c(88,99)] <-NA


#INCOME2 (income)

LLCP2014$income[LLCP2014$X_INCOMG==1] <-"<=$15,000"
LLCP2014$income[LLCP2014$X_INCOMG ==2] <-"$15,000-$25,000"
LLCP2014$income[LLCP2014$X_INCOMG ==3] <-"$25,000-$35,000"
LLCP2014$income[LLCP2014$X_INCOMG  ==4] <-"$35,000-$50,000"
LLCP2014$income[LLCP2014$X_INCOMG  ==5] <-"$5000 or more"
LLCP2014$income<- as.factor(LLCP2014$income)


#X_EDUCAG (education)

LLCP2014$Education[LLCP2014$X_EDUCAG %in% c(1,2)] <-"Less than College education"
LLCP2014$Education[LLCP2014$X_EDUCAG  %in% c(3,4)] <-"Have some college education or above"
LLCP2014$Education[LLCP2014$X_EDUCAG ==9] <-NA
LLCP2014$Education<- as.factor(LLCP2014$Education)

#"SLEPTIM1"
LLCP2014$sleepdep[LLCP2014$SLEPTIM1 %in% c(77,99)] <-NA
LLCP2014$sleepdep<-ifelse(LLCP2014$SLEPTIM1<7,"Yes","No")
LLCP2014$sleepdep<-as.factor(LLCP2014$sleepdep)
#SEX (gender)
LLCP2014$gender[LLCP2014$SEX==1] <-"Male"
LLCP2014$gender[LLCP2014$SEX==2] <-"Female"
LLCP2014$gender<- as.factor(LLCP2014$gender)

#"MARITAL"
LLCP2014$maritalstatus[LLCP2014$MARITAL ==1] <-"Married"
LLCP2014$maritalstatus[LLCP2014$MARITAL %in% c(2,3,4,6)] <-"Widowed/Divorced/Separated/Living with partner"
LLCP2014$maritalstatus[LLCP2014$MARITAL ==5] <-"Never Married"
LLCP2014$maritalstatus[LLCP2014$MARITAL ==9] <-NA
LLCP2014$maritalstatus<- as.factor(LLCP2014$maritalstatus)

#"WTKG3"
LLCP2014$weight<-as.numeric(LLCP2014$WTKG3)
LLCP2014$weight[LLCP2014$WTKG3==99999]<-NA

#"HTIN4"
LLCP2014$height<-as.numeric(LLCP2014$HTIN4)

#"ADDEPEV2",depressive disorder

LLCP2014$depression[LLCP2014$ADDEPEV2=="1"]<-"Yes"
LLCP2014$depression[LLCP2014$ADDEPEV2==2]<-"No"
LLCP2014$depression[LLCP2014$ADDEPEV2 %in% c(7,9)]<-NA
LLCP2014$depression<- as.factor(LLCP2014$depression)

LLCP2014<-na.omit(LLCP2014)
#sum(is.na(LLCP2014))#t
LLCP2014<-LLCP2014[,c("sleepdep","numchildren","gender","income","Age","Urbanity","maritalstatus","weight","Education","depression","height")]

```


Hold back about 30% of the data for testing data and 70% was training. 

```{r, message=F, warning=F, error=F,results="asis"}
set.seed(1)
train<-sample(nrow(LLCP2014),0.7*nrow(LLCP2014))
train.x<-LLCP2014[train,]
test.x<-LLCP2014[-train,]
train.y<-LLCP2014$depression[train]
test.y<-LLCP2014$depression[-train]
```


##Nonlinear models

```{r, message=F, warning=F, error=F,results="asis"}
library(splines2)
library(gam)
library(akima)
library(boot)
library(randomForest)
library(gbm)
library(e1071)
library(tree)
library(ROCR)
library(xtable)
```

In order to build a good model.I build the relationship of each variable to outcome, so we can see if the variable is linear or non-linear. 

###*height*

There are three ways to select the best model, we can build the model individual and use the anova with chisq or AIC to select the best model. Secondly, we can use cross-validation and the model gives the lowest test error is the best model.In here, we only show two methods, using AIC and test error. 

####Method 1

```{r, message=F, warning=F, error=F,results="asis"}
options(scipen = 200)
poly.aic <- NA
for(i in 1:5){
  poly.aic[i] <- AIC(glm(depression ~ poly(height, i), data = train.x,family = binomial))
}
poly.aic<-round(poly.aic,5)
which(poly.aic == min(poly.aic))
(upper<-min(poly.aic)+sd(poly.aic))
(lower<-min(poly.aic)-sd(poly.aic))
```

The AIC values are `r poly.aic`. According to one standard rule, the AIC ranges from [`r lower`,`r upper`]. The degree three of height will be enough to fit the model to prevent the overfitting.

####Method 2

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
cross.error=rep(0,5)
for (i in 1:5) {
  glm.poly<- glm(depression ~ poly(height, i), data = train.x,family = binomial)
  cross.error[i]<-cv.glm(train.x,glm.poly,K=5)$delta[1]
}
which.min(cross.error)
plot(1:5,cross.error,type = "l",ylab = "cross validation error",xlab = "degree of freedom")
abline(h=min(cross.error)+sd(cross.error))
abline(h=min(cross.error)-sd(cross.error))
```

From the plot, we can tell that height with a degree of 2 within the one standard rule.The result was not agree with the first method. Since, we want to pick the simple model,degree of 2 is good enough.

###weight

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
cross.error=rep(0,5)
for (i in 1:5) {
  glm.poly<- glm(depression ~ poly(weight, i), data = train.x,family = binomial)
  cross.error[i]<-cv.glm(train.x,glm.poly,K=5)$delta[1]
}
which.min(cross.error)
plot(1:5,cross.error,type = "l",ylab = "cross validation error",xlab = "degree of freedom")
abline(h=min(cross.error)+sd(cross.error))
abline(h=min(cross.error)-sd(cross.error))
```

According to the one starndar rule, it seems like three degree will give good decreasing in the test error. However, there is an interesting set. After we add two degree of polynominal, the test error was increased. After third degree, it was decreases. It indicates that the third polynominal maybe overfit. So in here, linear is good enough. 

###number of childern 

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
cross.error=rep(0,5)
for (i in 1:5) {
  glm.poly<- glm(depression ~ poly(numchildren, i), data = train.x,family = binomial)
  cross.error[i]<-cv.glm(train.x,glm.poly,K=5)$delta[1]
}
which.min(cross.error)
plot(1:5,cross.error,type = "l",ylab = "cross validation error",xlab = "degree of freedom")
abline(h=min(cross.error)+sd(cross.error))
abline(h=min(cross.error)-sd(cross.error))
```

Before we try to fit polynominal, we try different ways, like natural splines or smooth splines to fit the model. There is no difference using these method for number of children. 

Adding the third degree of number of children can decrease the test error. In here we found interesting situation that when we add the second order the test error was increasing, but if we adding more it was decreasing. The plot was similar to the height variable. It may shows there is a probability to ovefit. So In here, linear numchildren is enough of number of children variable to fit the model.

####Fitting model

Other variables were all categorical variable. It is hard to fit non-linear realtionship. So we begin fitting the model. 

*First, we add all the variables. Then we remove the non-significant variable one by one to see if it imporves the model's AIC values. We try to find the model gives the smallest AIC values. *

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
library(gam)
gam.fit.0<-gam(depression~poly(height,2)+sleepdep+gender+income+Age+Urbanity+
                 maritalstatus+Education+weight+numchildren,data = train.x,family = binomial(link = "logit"))
a<-summary(gam.fit.0)
AIC.full<-a$aic
```

Urbanity and education have no significant effect in the depression prediction. The AIC of the full model was `r AIC.full`.

We will rebuild the model, take each variable out and compare the AIC

Take the *Education* variable out.

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
gam.fit.0.0<-gam(depression~poly(height,2)+sleepdep+gender+income+Age+Urbanity+
                 maritalstatus+weight+numchildren,data = train.x,family = binomial(link = "logit"))
t<-anova(gam.fit.0.0,gam.fit.0,test = "Chisq")
p_1<-t$`Pr(>Chi)`[2]
AIC<-round(summary(gam.fit.0)$aic,4)
```

After we get rid of the education variable, the AIC was `r AIC`. It didn't change. The anova chisquare test shows that there is no significant adding the education variable.(P-value is `r p_1`)

Take the *urbanity* variable out. 

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
gam.fit.0.1<-gam(depression~poly(height,2)+sleepdep+gender+income+Age+
                 maritalstatus+Education+weight+numchildren,data = train.x,family = binomial(link = "logit"))
AIC<-summary(gam.fit.0.1)$aic
a<-anova(gam.fit.0.1,gam.fit.0,test = "Chisq")
p_2<-a$`Pr(>Chi)`[2]
```

There is no significant difference in adding the urbanity.The AIC is `r AIC`. It didn't change after we get rid of this variable. From the anova test, the p-value was `r p_2`. There is no significant difference. It means that Urbanity should be left out from the final model.

*Final Model*

As a conclusion, Urbanity and education should not be included in the final model for the generalized linear model method.The final model was in the following:

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
library(xtable)
library(knitr)
gam.fit.1<-gam(depression~poly(height,2)+sleepdep+gender+income+Age+
                 maritalstatus+weight+numchildren,data = train.x,family = binomial(link = "logit"))
a<-summary(gam.fit.1)
pvalue<-a$parametric.anova[5]
t<-data.frame(names=c("poly(height)","sleepdep","gender","income","Age","marital status",
                      "weight","number of children","residual"),pvalue=pvalue[1:9,])
print(xtable(t),type = "html")
```


##Trees 

Because we want to compare which tree method is better like *basic tree*, *random forest*, *bagging* and *boosting*. so we further split the data because we will use estimate test error to give a good estimation.

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
set.seed(1)
train.tree<-sample(nrow(train.x),0.7*nrow(train.x))
train.tree.x<-train.x[train.tree,]
tree.test.x<-train.x[-train.tree,]
tree.train.y<-train.x$depression[train.tree]
tree.test.y<-train.x$depression[-train.tree]

set.seed(1)
library(tree)
tree.brfss2014 <- tree(depression~height+sleepdep+gender+income+Age+
                    maritalstatus+weight+numchildren+Education+Urbanity,data= train.tree.x)
summary(tree.brfss2014)

# Show a plot of the tree with text() node labels
plot(tree.brfss2014)
text(tree.brfss2014, pretty = 0)

tree.pred <- predict(tree.brfss2014,tree.test.x, type = "class")
(brfss.table <- table(tree.pred, tree.test.y))
test.error<-sum(diag(brfss.table ))/sum(brfss.table)

cv.brfss2014<- cv.tree(tree.brfss2014, FUN = prune.misclass )
cv.brfss2014

par(mfrow = c(1, 2))
plot(cv.brfss2014$size, cv.brfss2014$dev, type = "b")
plot(cv.brfss2014$k, cv.brfss2014$dev, type = "b")
```

The testing error is `r test.error`. Gender and income are the important variables. It is improving from we predict them all have depression.
In here I try to prune the tree, But it shows it is not legitimate tree.It may because the tree is simple enough. There is no necessary to prune it.If we see the plot, we can see that the tree only gives two variables. The deviance does not change when the node or size changes.

Also, I tried different method using "gini". It shows that there is no change of the deviance when the size or node changes.

###Bagging

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
library(randomForest)
set.seed(1)
bag.brfss2014 <- randomForest(depression ~ ., data = train.tree.x, mtry = 10, importance = TRUE)
bag.brfss2014 

predict.bag <- predict(bag.brfss2014 , newdata = tree.test.x,type = "response")
(brfss.table <- table(predict.bag, tree.test.y))
test.error<-1-sum(diag(brfss.table ))/sum(brfss.table)
importance(bag.brfss2014)
varImpPlot(bag.brfss2014)
```


The test error is about `r test.error`. it seems like weight, gender,height and income are the the imporatnt variables that can decrease the accurancy. 

###Random forest

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
set.seed(1)
rf.brfss2014 <- randomForest(depression ~ ., data = train.tree.x, mtry = 3, importance = TRUE)
rf.brfss2014 
predict.bag <- predict(rf.brfss2014 , newdata = tree.test.x,type = "response")
(brfss.table <- table(predict.bag, tree.test.y))
te<-1-sum(diag(brfss.table ))/sum(brfss.table)
importance(rf.brfss2014)
varImpPlot(rf.brfss2014)
```

The test error is about `r te`. it seems like weight, gender,height and income are the the imporatnt variables that can decrease the accurancy. The result is similar to bagging. 


###boosting

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
library(gbm)
set.seed(1)
train.tree.x$depression<-as.numeric(train.tree.x$depression)
train.tree.x$depression<-train.tree.x$depression-1
boost.brfss2014 <- gbm(depression ~ ., data = train.tree.x, distribution = "bernoulli", n.trees = 5000,
                       verbose = F)
summary(boost.brfss2014)
par(mfrow = c(1, 2)) 
plot(boost.brfss2014, i = "income") 
plot(boost.brfss2014, i = "gender")

tree.test.x$depression<-as.numeric(tree.test.x$depression)
tree.test.x$depression<-tree.test.x$depression-1
#it will return probability for bernuolli 
pred.boost.pred <- predict(boost.brfss2014, newdata = tree.test.x, n.trees = 5000,type="response")
pred <- rep("Yes", length(pred.boost.pred ))
pred[pred.boost.pred  < 0.5] <- "No"

(brfss.table <- table(pred, tree.test.y))
te<-1-sum(diag(brfss.table ))/sum(brfss.table)
```


It seems like income and gender are important variables.and the test error is `r te`.

**Summary**
All in all the basic tree and boost tree gives the lowest test error. The basic tree is more clearly showing the relationship. Their important variables are quite similar. Both shows that the income and gender are important variables. But tree is simple, faster in terms of computation time, the plot is easy to understand. 

## SVM (4 points)

###linear separate

Because it takes a long time to run a svm using different cost number. It takes forever in my
computer to run tune function with a range of cost, so I just run them differently.!!

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
library(e1071)
set.seed(1)
#cost=0.01
brfss2014.svm <- svm(depression ~ ., data = train.tree.x, kernel="linear",
                 cost=0.01)
summary(brfss2014.svm)

ypred <- predict(brfss2014.svm, tree.test.x)
svm.table<-table(predict = ypred, truth = tree.test.y)
te.l.1<-sum(diag(svm.table))/sum(svm.table)#0.8067863
```


```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
#cost=0.1
brfss2014.svm <- svm(depression ~ ., data = train.tree.x, kernel="linear",
                     cost=0.1,scale = FALSE)
summary(brfss2014.svm)

ypred <- predict(brfss2014.svm, tree.test.x)
svm.table<-table(predict = ypred, truth = tree.test.y)
sum(diag(svm.table))/sum(svm.table)#0.8067863
```


The test error seems no change using cost 0.01,0.1 and 1. They are `r te.l.1`. But it takes forever to tune the svm. It also take a while to use the radial method. So in here, I only show two cost. I will use 0.1 in the following kernel.

###kernel

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
brfss2014.svm <- svm(depression ~ ., data = train.tree.x, kernel="radial",
                     cost=0.1,gamma=1,scale = FALSE)
summary(brfss2014.svm)

ypred <- predict(brfss2014.svm, tree.test.x)
svm.table<-table(predict = ypred, truth = tree.test.y)
sum(diag(svm.table))/sum(svm.table)#0.8067863
```

The two methods show same results. There is no significant change in the testing error. In here we use the linear method with cost=0.1, because it faster!!! The test error is `r te.l.1`.

## Model Evaluation (4 points)

###nonlinear method

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
gam.fit<-gam(depression~poly(height,2)+sleepdep+gender+income+Age+
             maritalstatus+weight+numchildren,data=test.x,family = binomial(link = "logit"))

prob<-predict(gam.fit,test.x,type = "response")
preds<-rep("No",nrow(test.x))
preds[prob>0.5]<-"Yes"
(gam.table <- table(preds, test.y))
te<-1-sum(diag(gam.table ))/sum(gam.table)

#The specificity =true negative(12223)/(true negative 12223+false positive(133))
sp<-12223/(12223+133)
#0.98923

#The sensitivity=true positive/total positive=0.0930
se<-277/(277+2700)

#accuracy
ac<-(12223+227)/(2700+133+12223+227)#0.8146306

#ROC curve:
predob <- prediction (prob, test.y)
perf <- performance(predob , "tpr", "fpr")
plot(perf,main="ROC curve")
#It seems like only continue variable was support 
```


The test error was `r te`, the specificity was `r sp`, the sensitivity was `r se` and the accuracy was `r ac`. The accuracry was same as test error. The ROc curve was above. 

###tree:basic tree

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
tree.brfss2014 <- tree(depression~height+sleepdep+gender+income+Age+
                         maritalstatus+weight+numchildren+Education+Urbanity,data= test.x)
summary(tree.brfss2014)

# Show a plot of the tree with text() node labels
plot(tree.brfss2014)
text(tree.brfss2014, pretty = 0)

#It seems like the tree predict all the outcome as No

tree.pred <- predict(tree.brfss2014,test.x, type = "class")
(brfss.table <- table(tree.pred,test.y))
sum(diag(brfss.table ))/sum(brfss.table)
#0.80848
#The specificity =true negative(12356)/(true negative 12356+false positive 0)
#1
#The sensitivity=true positive/total positive=0
0
#accuracy
ac<-(12356)/(12356+2927)#0.80848
```

I was not be able to make the ROC curve of it. The tree predict all the variables to "No". As we mentioned. If we characterized all to "No". The specificity was 1 and the sensitivity was 0. While the test error and the accruacry was `r ac`. It was not good. 

###Support Vector Machine

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
svmfit.opt <- svm(depression ~ ., data = test.x, kernel = "linear", 
                  cost =0.1, decision.values = T)

fitted <- attributes(predict(svmfit.opt, test.x, 
                             decision.values = TRUE))$decision.values
rocplot <- function(pred, truth, ...){
  predob <- prediction (pred, truth)
  perf <- performance(predob , "tpr", "fpr")
  plot(perf ,...)
}

rocplot(fitted, test.y, main = "testing Data")

ypred <- predict(svmfit.opt, test.x)
svm.table<-table(predict = ypred, truth =test.y)
sum(diag(svm.table))/sum(svm.table)#0.80848
#The specificity =true negative(12356)/(true negative 12356+false positive 0)
#1
#The sensitivity=true positive/total positive=0
0
#accuracy
(12356)/(12356+2927)#0.80848
```

It seems like the SVM method also predict all the outcome as No. It is interesting that the results was same as tree. The specificity was 1 and the sensitivity was 0. While the test error and the accruacry was `r ac`. It was not good. However, the ROC plot was worse than 0.5.

*As a conclusion,the best model was non-linear model. It gives a little bit better improvement of the prediction of the deression to all "No", like 1% improvement. It gives the highest accuracry and lowest test error.* 

## Unsupervised learning (4 points)

*Use a heirarchical clustering algorithm to find two clusters in the data. How well do those two clusters conform to your binary outcome variable? What might this mean for the potential relationship between our variables and the outcome?*

```{r, message=F, warning=F, error=F,results="asis",fig.width=16, fig.height=6,fig.align="center"}
set.seed(1) 
c.train<-sample(nrow(LLCP2014),1000)
c.train.x<-LLCP2014[c.train,]
c.train.y<-LLCP2014$depression[c.train]
x<-as.matrix(c.train.x)#we can't scale it because we have two categorical variables.
hc.complete <- hclust(dist(x), method = "complete")
hc.average <- hclust(dist(x), method = "average")
hc.single <- hclust(dist(x), method = "single")


par(mfrow = c(1, 3))
plot(hc.complete, main = "Complete Linkage", xlab = "", 
     sub = "", cex = 0.9)
plot(hc.average, main = "Average Linkage", xlab = "", 
     sub = "", cex = 0.9)
plot(hc.single, main = "Single Linkage", xlab = "", 
     sub = "", cex = 0.9)

hc.cl.clusters<-cutree(hc.complete, 2)
(c.table<-table(hc.cl.clusters, c.train.y))
ac.cl<-sum(diag(c.table))/sum(c.table)#The accuracy is only 0.48

hc.av.clusters<-cutree(hc.average, 2)
(av.table<-table(hc.av.clusters, c.train.y))
ac.av<-sum(diag(av.table))/sum(av.table)
#0.76

hc.sg.clusters<-cutree(hc.single, 2)
(sl.table<-table(hc.sg.clusters, c.train.y))
ac.sl<-sum(diag(sl.table))/sum(sl.table)#0.8

```


Because using the whole data set will take a lot of memeory to run.We just pick 10000 samples, about 20%, to run the model.From the table and results, we can see that the accuracry or the training error for complete method, the accuracry is `r ac.cl`. For average method, it was `r ac.av`. for single method, it was `r ac.sl`. The single method gives the highest accuracy. It kind of agree with the previous results that only few variables were associated with the depression. That's why maybe single method was compete out the other methods.But it is still not a good prediction model. 
















































