#support vector classifier
#the svm function can be used to fit a support vector classifier when the argument kernel="linear" is used. the 
#cost argument allows us to specify the cost of a violation to the margin. when the cost argument is large, then the margins will be
#narrow and there will be few support vectors on the margin or violating the margin
set.seed(1)
x=matrix(rnorm(20*2),ncol=2)
y=c(rep(-1,10),rep(1,10))
x[y==1,]=x[y==1,]+1
#begins by 
plot(x,col=(3-y))
#in order for the svm function to perform classification, encode the response as a factor variable
dat=data.frame(x=x,y=as.factor(y))
library(e1071)
svmfit=svm(y~.,data=dat,kernel="linear",cost=10,scale=F)
#the scale=F tells the svm function not to scale each feature to have mean zero or standard deviation one
plot(svmfit,dat)
#the decision boundary between the two classes is linear because kernel="linear"
svmfit$index
summary(svmfit)

svmfit=svm(y~.,data=dat,kernel="linear",cost=0.1,scale=F)
plot(svmfit,dat)
svmfit$index
#we obtain a larger number of support vectors, because the margin is now wider. 
#the e1071 includes a built-in function,tune() to perform ten-fold cros-validation.

#the following command indicates that we want to compare SVMs with a linear kernel using a range of values of the cost

set.seed(1)
tune.out=tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))
summary(tune.out)
#we see that cost=0.1 results in the lowest coss-validation error rate. the tune() function stores the best model
#obtained
bestmod=tune.out$best.model
summary(bestmod)
#the opredict function can be used to predict the class lable on a set of test observations, at any given value of the cost parameter
xtest=matrix(rnorm(20*2),ncol=2)
ytest=sample(c(-1,1),20,rep=T)
xtest[ytest==1,]=xtest[ytest==1,]+1
testdata=data.frame(x=xtest,y=as.factor(ytest))
#we predict the class labels of these test observations, at a given value of the cost parameter
ypred=predict(bestmod,testdata)
table(predict=ypred,truth=testdata$y)
#19 of the test observations are correctly classified. 
svmfit=svm(y~.,data=dat,kernel="linear",cost=0.01,scale=F)
ypred=predict(svmfit,testdata)
table(predict=ypred,truth=testdata$y)
#in this one, one addition obs is missclassified

#now consider a situation in which the two classes are linearly separable
#we find a separating hyperplane using the svm function.
x[y==1,]=x[y==1,]+0.5
plot(x,col=(y+5)/2,pch=19)
#we fit the support vecotr classifier and plot the resulting hyperplane, using a very large value of cost so that no obs are misclassified
dat=data.frame(x=x,y=as.factor(y))
svmfit=svm(y~.,data=dat,kernel="linear",cost=1e5)
summary(svmfit)
plot(svmfit,dat)
#No training error were made and only three support vectors were used. the margin is very narrow,
#because the observatons that are not support vectors, indicated as circles are very close to decision boundary
# it seems likely that this model will perform poorly on test data


svmfit=svm(y~.,data=dat,kernel="linear",cost=1)
summary(svmfit)
plot(svmfit,dat)

#using cost=1, we misclassify a training observation, but we obtain a much wider margin and make use of severn support vectors

#Support Vector Machine
#fit an SVM using a non-linear kernel, we once again use the svm kernel="polynomial"; use kernel="radial" to fit an SVM with a radial kernel
set.seed(1)
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1:150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))

plot(x,col=y)
#the data is randomly split into training and testing groups. We then fit the training data using the svm() with
#a radial kernel and gamma=1
train=sample(200,100)
svmfit=svm(y~.,data=dat[train,],kernel="radial",gamma=1,cost=1)
plot(svmfit,dat[train,])

#the plot shows that the resulting SVM has a decidedly non-linear boundary. 
summary(svmfit)
#there are a fair number of training errors in this SVM fit. if we increase the cost, we can reduce the number of
#training errors, However, this comes at the price of a more irregular decision boundary that seems to be at
#risk of overfitting the data
svmfit=svm(y~.,data=dat[train,],kernel="radial",gamma=1,cost=1e5)
plot(svmfit,dat[train,])

#we can perform cross-validation using tune() to select the best choice of gamma and cost for an SVM with a radial kernel
set.seed(1)
tune.out=tune(svm,y~.,data=dat[train,],kernel="radial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out)
#therefore, the best choice of parameters involves cost=1 and gamma=2. 

table(true=dat[-train,"y"],pred=predict(tune.out$best.model,newx=dat[-train,]))


#ROC curves
library(ROCR)
#we first write a short function to plot an ROC curve given a vector containing a numerical score for each
#obervation, pred and a vector containing the class label for reach obs, truth

rocplot=function(pred,truth, ...){
  predob=prediction(pred,truth)
  perf=performance(predob,"tpr","fpr")
  plot(perf,...)
}

#in order to obtain the fitted values for a given SVM model fit, we use decision.values=T when fitting svm()
#then predict() function will output the fitted values
svmfit.opt=svm(y~.,data=dat[train,],kernel="radial",gamma=2,cost=1,decision.values=T)
fitted=attributes(predict(svmfit.opt,dat[train,],decision.values=T))$decision.values
par(mfrow=c(1,2))
rocplot(fitted,dat[train,"y"],main="Training Data")

#increasing gamma we can produce a more flexible fit and generate further improvements in accuracy

svmfit.flex=svm(y~.,data=dat[train,],kernel="radial",gamma=50,cost=1,decision.values=T)
fitted=attributes(predict(svmfit.flex,dat[train,],decision.values=T))$decision.values
par(mfrow=c(1,2))
rocplot(fitted,dat[train,"y"],add=T,col="red")


fitted=attributes(predict(svmfit.opt,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],main="Test Data")

fitted=attributes(predict(svmfit.flex,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],add=T,col="red")

#SVM with multiple classes
#the svm function will perform multi-class classification using the one-versue-one approach
set.seed(1)
x=rbind(x,matrix(rnorm(50*2),ncol=2))
y=c(y,rep(0,50))
x[y==0,2]=x[y==0,2]+2
dat=data.frame(x=x,y=as.factor(y))

par(mfrow=c(1,1))
plot(x,col=(y+1))
svmfit=svm(y~.,data=dat,kernel="radial",cost=10,gamma=1)
plot(svmfit,dat)

#application to gene data
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytest)
length(Khan$ytrain)

#the data set consists of expression measurements for 2308. The training and test sets consist of 63 and 20 
#observations
table(Khan$ytrain)
table(Khan$ytest)


#there are a very large number of features relative to the number of observations. This suggests that we should use a linear knerl
#because the additional flexibility that will result from using a polynomial or radial kernel is unnecessary
dat=data.frame(x=Khan$xtrain,y=as.factor(Khan$ytrain))
out=svm(y~.,data=dat,kernel="linear",cost=10)
summary(out)

table(out$fitted,dat$y)
#there are no training errors. It is not suprising, because the large number of variables relative to the number of
#observations implies that it is easy to find hyperplanes that fully separate the classes. '
#we interested the its performance on the test observations

dat.te=data.frame(x=Khan$xtest,y=as.factor(Khan$ytest))
pred.te=predict(out,newdata=dat.te)
table(pred.te,dat.te$y)
#using cost 10 yeilds two test error on this data






















































