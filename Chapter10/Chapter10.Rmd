---
title: "Chapter10"
author: "Wenhui Zeng"
date: "February 14, 2017"
output: pdf_document
---

#1

##a

$$\frac{1}{|C_k|}\sum_{i,i'\in C_k}\sum_{j=1}^{p}((x_{ij}-\bar{x}_{kj})-(x_{i'j}-\bar{x}_{kj}))^2$$
$$=\frac{1}{|C_k|}\sum_{i,i'\in C_k}\sum_{j=1}^{p}((x_{ij}-\bar{x}_{kj})^2-2(x_{ij}-\bar{x}_{kj})(x_{i'j}-\bar{x}_{kj})+(x_{i'j}-\bar{x}_{kj})^2)$$
$$\sum_{i,i'\in C_k}=|C_k|\sum_{i\in C_k}$$
So the equation can become 
$$=\frac{|C_k|}{|C_k|}\sum_{i\in C_k}\sum_{j=1}^{p}(x_{ij}-\bar{x}_{kj})^2+\frac{|C_k|}{|C_k|}\sum_{i'\in C_k}\sum_{j=1}^{p}(x_{i'j}-\bar{x}_{kj})^2-2\frac{1}{|C_k|}\sum_{i,i'\in C_k}\sum_{j=1}^{p}(x_{ij}-\bar{x}_{kj})(x_{i'j}-\bar{x}_{kj})$$

Because $i,i'\in C_k$ so they are actually the same
Also$$\bar{x}_{kj}=\frac{1}{|C_k|}\sum_{i \in C_k}x_{ij}$$



$$2\sum_{i\in C_k}\sum_{j=1}^{p}(x_{ij}-\bar{x}_{kj})^2$$

##b

Equation shows that minimizing the sum of the squared Euclidean distance for 
each cluster is the same as minimizing variance for each cluster

#2

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
d = as.dist(matrix(c(0, 0.3, 0.4, 0.7, 
                     0.3, 0, 0.5, 0.8,
                     0.4, 0.5, 0.0, 0.45,
                     0.7, 0.8, 0.45, 0.0), nrow=4))
plot(hclust(d, method="complete"))
plot(hclust(d, method="single"))

```

##c

If the results in (a) have two cluster. We can see that (1,2) and (3,4)

##d

The two clusters are (4), (1,2,3)

##e
```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
plot(hclust(d, method="complete"), labels=c(2,1,4,3))
```

#3

##a
```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
set.seed(1)
x1<-c(1,1,0,5,6,4)
x2<-c(4,3,4,1,2,0)
data<-cbind(x1,x2)
plot(data[,1],data[,2])
plot(data)
```

##b
```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
s<-sample(2,nrow(data),replace = T)
plot(data[,1],data[,2],col=s+1,pch=20,cex=2)
```

##c
```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
centroid1<-c(mean(data[s==1,1]),mean(data[s==1,2]))
centroid2<-c(mean(data[s==2,1]),mean(data[s==2,2]))
plot(data[,1],data[,2],col=s+1,pch=20,cex=2)
points(centroid1[1],centroid1[2],col=1,pch=4)
points(centroid2[1],centroid2[2],col=2,pch=4)
```

##d
```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
s<-c(1,1,1,2,2,2)
plot(data[,1],data[,2],col=s+1,pch=20,cex=2)
points(centroid1[1],centroid1[2],col=1,pch=4)
points(centroid2[1],centroid2[2],col=2,pch=4)
```

##e
```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
centroid1<-c(mean(data[s==1,1]),mean(data[s==1,2]))
centroid2<-c(mean(data[s==2,1]),mean(data[s==2,2]))
plot(data[,1],data[,2],col=s+1,pch=20,cex=2)
points(centroid1[1],centroid1[2],col=1,pch=4)
points(centroid2[1],centroid2[2],col=2,pch=4)
```

In here each observation was assigned to the closest centroid, the iteration was stopped

#f
```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
plot(data[,1],data[,2],col=s+1,pch=20,cex=2)
```

Point 1,2,3 was in one cluster and point 4,5,6 in another

#4

##a

There is not enough information to tell. For example, if $d(1,4) = 4$, $d(1,5) = 6$, $d(2,4) = 2$, $d(2,5) = 4$, $d(3,4) = 5$ and $d(3,5) = 3$, the single linkage dissimilarity between ${1,2,3}$ and ${4,5}$ would be equal to 2, because it computes all the pairwise distance and record the smallest and the complete linkage dissimilarity between ${1,2,3}$ and ${4,5}$ would be equal to 6.It computes all the pairwise distance and record the largest dissimilarities. So, with single linkage, they would fuse at a height of 2, and with complete linkage, they would fuse at a height of 6.  But, if all inter-observations distance are equal to 4, we would have that the single and complete linkage dissimilarities between ${1,2,3}$ and ${4,5}$ are equal to 4. There is not enough informaiton


##b

They will fuse at the same height. Because we only have one distance measurement between ${5}$ and ${6}$. 

#5

##a

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
socks<-c(10,9,8,7,6,5,10)
computers<-c(0,0,0,1,1,1,1)
shop<-cbind(socks,computers)
labels<-c(1,1,1,2,2,2,1)
plot(shop[,1],shop[,2],col=labels+1,pch=20,cex=2)
```

##b
```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
socks<-scale(socks,center = F)
computers<-scale(computers,center = F)
shop<-cbind(socks,computers)
labels<-c(1,1,1,2,2,2,1)
plot(shop[,1],shop[,2],pch=20,cex=2)
```

If we consider the variables measured by the money they spend,the scale variables, the computer will play a big role. If we take the unscaled variables, the number of socks played a important role.

#6

##a

The first principal component explains 10% of the information. There are about 90% of the information was lost by projecting the tissue sample observation onto the first principal component.

##b

In the pre-analysis, the first principle component has a strong linear trend from left to right. Also, each patient sample was run on one of two machines, A and B and machine A was used more often in the earlier times while B was used more often later. It is better to inlude the machine used A vs B as a feature of the data set. It might enhance the principle variation explained. 

##c

#c
```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
set.seed(10)
control<-matrix(rnorm(50*1000),ncol = 50)
treatment<-matrix(rnorm(50*1000),ncol = 50)
data<-cbind(control,treatment)
data[1,]<-seq(-20,20-0.4,.4)#Linear trend in one dimension

prc<-prcomp(scale(data))
summary(prc)$importance[,1]

#0.11541 explained by the first principle

#Now addint A vs B via 10 vs 0 coding

data<-rbind(data,c(rep(10,50),rep(0,50)))
prc<-prcomp(data,scale=T)
summary(prc)$importance[,1]

```

Before adding the A vs B, 0.11541 explained by the first principle, after we adding the variable, 0.147900  of the variables was explained.It was showing that adding the A vs B
improving the variation been explained. 

```{r}
make.grid<-function(x,n=75){
  grange=apply(x,2,range)
  x1=seq(from=grange[1,1],to=grange[2,1],length=n)
  x2=seq(from=grange[1,2],to=grange[2,2],length=n)
  expand.grid(X1=x1,X2=x2)
}
```


#7

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
library(ISLR)
set.seed(2)
scal<-scale(USArrests)
r_ij<-cor(t(scal))
a<-dist(scal)^2
b<-as.dist(1-r_ij)
summary(a/b)
```

each obs. has been centered to have mean zero and 
standard deviation one. 

#8

##a

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
set.seed(1)
pr.out<-prcomp(USArrests,scale=T,center=T)
#or scale(USArrest)
pr.var<-pr.out$sdev^2
pve1<-pr.var/sum(pr.var)
```

The principle variance explained was `r pve1`.

##b
```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
#loading
loading<-pr.out$rotation
data<-scale(USArrests)
sumvar<-sum(apply(as.matrix(data)^2,2,sum))
#same as sum(apply(as.matrix(data)^2,1,sum))
pve2<-apply((as.matrix(data) %*% loading)^2,2,sum)/sumvar
```

The principle variance explained was `r pve2`.

#9

##a

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
hc.complete<-hclust(dist(USArrests),method = "complete")
summary(hc.complete)

plot(hc.complete, main = "Complete Linkage with Euclidean Distance", xlab = "", sub = "")
tree.0<-cutree(hc.complete,3)
table(tree.0)
```

##c

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
data.usarrest<-scale(USArrests)
hc.complete.scale<-hclust(dist(data.usarrest),method = "complete")
summary(hc.complete.scale)

plot(hc.complete.scale, main = "Complete Linkage with Euclidean Distance", xlab = "", sub = "")
tree.1<-cutree(hc.complete.scale,3)
table(tree.1)
```

##d

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
table(tree.0,tree.1)
```

Scaling the variables effects the max height of the dendogram obtained 
from hierarchical clustering. It does affect the 
clusters obtained from cutting the dendogram into 3 clusters. The data should be standardized
and center.Because different variables meansured on different scale. If we didn't standardize,
some variables may dominate the effect.

#10

##a
```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
set.seed(10)
data<-matrix(rnorm(20*3*50,mean=0,sd=0.001),ncol = 50)

data[1:20, 2] <- -10
data[21:40, 1] <- 2
data[21:40, 2] <- 2
data[41:60, 1] <- 10
true.labels<-c(rep(1, 20), rep(2, 20), rep(3, 20))
#to check the separation
plot(data)
```

##b

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
pc.out<-prcomp(data,scale=T)
summary(pc.out)
plot(pc.out$x[,1:2],col=2:4,pch=19,xlab = "Z1",ylab = "Z2")
```

##c

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
km.out<-kmeans(data,3,nstart=20)
table(true.labels,km.out$cluster)
```

The observation was cluster correctly.

##d

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
km.out<-kmeans(data,2,nstart=20)
table(true.labels,km.out$cluster)
```

The first class contain two cluster

##e

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
km.out<-kmeans(data,4,nstart=20)
table(true.labels,km.out$cluster)
```

The k-means method cluster 1 and 3 was all belong to one true cluster.

##f

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
km.out<-kmeans(pc.out$x[,1:2],3,nstart=20)
table(true.labels,km.out$cluster)
```

The results were worse than other method.

##g

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
data.scale<-scale(data)
km.out<-kmeans(data,3,nstart=20)
table(true.labels,km.out$cluster)
```

The data was separated correctly.There is no bad effect

#11

##a

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
ch10 <- read.csv('D:/courses/BSTcourse/machine learning and predictive modeling/week11/Ch10Ex11.csv', header=F)

#b
ch10_scale<-scale(ch10)
dd <- as.dist(1 - cor(t(ch10)))
plot(hclust(dd, method = "complete"), main = "Complete Linkage with Correlation-Based Distance", xlab = "", sub = "")

plot(hclust(dd, method = "single"), main = "Single Linkage with Correlation-Based Distance", xlab = "", sub = "")

plot(hclust(dd, method = "average"), main = "Average Linkage with Correlation-Based Distance", xlab = "", sub = "")
```

Yes, The results were depend on the method we used. 

##c

Principle Component Aanalysis was used to see which genes differ the most. 
We will examine the absolute values of the total loadings for each gene as 
it characterizes the weight of each gene.

```{r,echo = F, message=F, warning=F, error=F,fig.width=16, fig.height=6}
pr.out<-prcomp(t(ch10))
head(pr.out$rotation)
total.load <- apply(pr.out$rotation, 1, sum)
index <- order(abs(total.load), decreasing = TRUE)
index[1:10]
```

There are two most different genes across the two groups.





































