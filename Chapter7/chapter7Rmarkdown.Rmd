---
title: "Chapter 7"
author: "Wenhui Zeng"
date: "February 12, 2017"
output: pdf_document
---
#1

##a

for $x \leq \xi$ $f(x)$ has coefficients that $a_1=\beta_0$, $b_1=\beta_1$ $c_1=\beta_2$ $d_1=\beta_3$
$$f(x)_1=\beta_0+\beta_1x+\beta_2x^2+\beta_3x^3$$




##b

if $x \geq \xi$ $f(x)$ has the form of $f(x)_1=\beta_0+\beta_1x+\beta_2x^2+\beta_3x^3$



So $$f(x)_2=\beta_0+\beta_1x+\beta_2x^2+\beta_3x^3+\beta_4(x^3-3x^2\xi+3x^2\xi-\xi^3)$$


$$(\beta_0-\beta_4\xi^3)+(\beta_1+3\beta_4\xi^2)x+(\beta_2-3\beta_4\xi)x^2+(\beta_4+\beta_3)x^3$$
So $$a_2=\beta_0-\beta_4\xi^3, b_2=\beta_1+3\beta_4\xi^2, c_2=\beta_2-3\beta_4\xi, 
 d_2=\beta_4+\beta_3$$

##c

$$f(\xi)_1=\beta_0+\beta_1\xi+\beta_2\xi^2+\beta_3\xi^3$$

$$f(x)_2=\beta_0+\beta_1x+\beta_2x^2+\beta_3x^3+\beta_4(x-\xi)_+^3$$
$$f(\xi)_2=\beta_0+\beta_1\xi+\beta_2\xi^2+\beta_3\xi^3=f(\xi)_1$$

##d

$$f(x)'_1=\beta_1+2\beta_2x+3\beta_3x^2$$


$$f(\xi)'_1=\beta_1+2\beta_2\xi+3\beta_3\xi^2$$

$$f(x)'_2=\beta_1+2\beta_2x+3\beta_3x^2+3\beta_4(x-\xi)_+^2$$
$$f(\xi)'_2=\beta_1+2\beta_2\xi+3\beta_3\xi^2=f(\xi)'_1$$

##e

$$f(x)'_1=\beta_1+2\beta_2x+3\beta_3x^2$$

$$f(x)''_1=2\beta_2+6\beta_3x$$

$$f(\xi)''_1=2\beta_2+6\beta_3\xi$$


$$f(x)''_2=2\beta_2+6\beta_3x+6\beta_4(x-\xi)_+$$
$$f(\xi)''_2=2\beta_2+6\beta_3\xi=f(\xi)''_1$$

#2

Suppose that a curve $\hat{g}$ is computed to smoothly fit a set of n points using the following formula:

$$\hat{g}=\arg \min_g(\sum_{i=1}^{n}(y_i-g(x_i))^2+\lambda\int[g^{m}(x)]^2{dx})$$

where $g^{m}$ represents the *m*th dereviative of *g* > provide exmaple sketches of $\hat{g}$ in each of the following scenarios

##a

$\lambda=\infty, m=0$ 

$g(x) \propto x$ because RSS term is ignored and $g(x)'=k$ would minimize the area under the curve of $g(0)=g$.

##b

$\lambda=\infty, m=1$

$g(x)\propto x^2$ $g(x)$  would be quadratic to minimize the area under the curve of its first derivative.

##c

$\lambda=\infty, m=2$

$g(x)\propto x^3$ $g(x)$  would be cubic to minimize the area under the curve of its second derivative.


##d

$\lambda=\infty, m=3$

$g(x)\propto x^4$ $g(x)$  would be quartic to minimize the area under the curve of its third derivative.

##e

The penalty term no longer matter. This is the formula for linear regression, to choose g based on minimizing RSS.


#3

Suppose we fit a curve with basis functions $$b_1(x) =x, b_2(x)=(x-1)^2I(x \geq 1) $$ equals 1 for $x \geq 1$ and 0. 
We fit a linear regression model

$y=\beta_0+\beta_1b_1(x)+\beta_2b_2(x)+\epsilon$

and obtain coefficient estimates $\hat{\beta_0}=1,   \hat{\beta_1}=1, \hat{\beta_2}=-2$
```{r,echo=F,warning=F}
x<- -2:2
y=1+x-2*(x-1)^2*I(x>1)
plot(x,y)
```


#4

Suppose we fit a curve with basis functions $$b_1(x) =I(0 \leq x \leq 2)-(x-1)I(1 \leq x \leq 2 ), b_2(x)=(x-3)^2I( 3 \leq x \leq 4)+I(4 \leq x \leq 5) $$ equals 1 for $x \geq 1$ and 0. 
We fit a linear regression model

$$y=\beta_0+\beta_1b_1(x)+\beta_2b_2(x)+\epsilon$$

and obtain coefficient estimates $\hat{\beta_0}=1,   \hat{\beta_1}=1, \hat{\beta_2}=3$

```{r,echo=F,warning=F}
x = -2:2
y = c(1 + 0 + 0, # x = -2
      1 + 0 + 0, # x = -1
      1 + 1 + 0, # x = 0
      1 + (1-0) + 0, # x = 1
      1 + (1-1) + 0 # x =2
      )
plot(x,y)
```



#5

$$\hat{g}_1=\arg \min_g(\sum_{i=1}^{n}(y_i-g(x_i))^2+\lambda\int[g^{3}(x)]^2{dx})$$


$$\hat{g}_2=\arg \min_g(\sum_{i=1}^{n}(y_i-g(x_i))^2+\lambda\int[g^{4}(x)]^2{dx})$$

where $g^{m}$ represents the *m*th dereviative of *g* 

##a



As $\lambda=\infty$ 
We'd expect $\hat{g}_2$ to have the smaller training RSS because it will be a higher order polynomial due to the order of the derivative penalty function.

##b

We'd expect $\hat{g}_1$ to have the smaller test RSS because $\hat{g}_2$ could overfit with the extra degree of freedom.

##c
when $\lambda=0$ it will become linear regression. 


#6

##a
```{r,echo=F,warning=F}
library(ISLR)
library(boot)
par(mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
set.seed(14)
#since if the polynominal is greater than 4 or 5 tends to overfit, so we just test upto 5-degree
cross.error=rep(0,5)
for (i in 1:5) {
  lm.poly<- glm(wage~poly(age,i),data=Wage)
  cross.error[i]<-cv.glm(Wage,lm.poly,K=5)$delta[1]
}

which.min(cross.error)
```

Since if the polynominal is greater than 4 or 5 tends to overfit, so we just test upto 5-degree
cross.error=rep(0,5). The minimum is degree of 5, but we can see the test error didn't change much  from degree 3 to 5. So I will choose degree of 3 as the optimal one


```{r,echo=F,warning=F}
cross.error=rep(0,5)
plot(1:5,cross.error,type = "l",ylab = "cross validation error",xlab = "degree of freedom")

#using ANOVA
lm<-glm(wage~age,data=Wage)
lm.fit.2<-glm(wage~poly(age,2),data = Wage)
lm.fit.3<-glm(wage~poly(age,3),data = Wage)
lm.fit.4<-glm(wage~poly(age,4),data = Wage)
lm.fit.5<-glm(wage~poly(age,5),data = Wage)

#method 1-ANOVA
anova(lm,lm.fit.2,lm.fit.3,lm.fit.4,lm.fit.5,test = "Chisq")
#The anova results indicate that the degree of 3 is sufficient. The degree of 4 is kind of at the critical point


#make a plot of the resulting polynominal fit to the data

agelimits<-range(Wage$age)

#the starting and (maximal) end values of the sequence. Of length 1 unless specify
age.range<-seq(from=agelimits[1],to=agelimits[2])
preds<-predict(lm.fit.3,newdata=list(age=age.range),se=T)
band<-cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)

#A numerical vector of the form c(bottom, left, top, right)

par(mfrow=c(1,1))
with(Wage,plot(age,wage,xlim=agelimits,cex=.5,col="darkgrey"))

title("Degree-3 Polynomial",outer = T)

with(Wage,lines(age.range,preds$fit,lwd=2,col="blue"))

matlines(age.range,band,lwd=1,col="blue")
```


##b

fit a step function to predict wage using age and perform cross-validation to choose the optimal number of cuts

```{r,echo=F,warning=F}
#i is the interval cut into i interval, there is no 1 interval
#corss-validation only can be used glm object, even it was linear use glm without specify family

#METHOD 1
step.cv<-rep(NA,10)
for (i in 2:10) {
  Wage$age.cut<-cut(Wage$age,i)
  fit.step<-glm(wage~age.cut,data = Wage)
  step.cv[i]<-cv.glm(Wage,fit.step,K=10)$delta[1]
}


which.min(step.cv)

#Method 2
step.cv<-rep(NA,10)
for (i in 2:10) {
  
  fit.step<-glm(wage~cut(age,i,labels = FALSE),data = Wage)
  step.cv[i]<-cv.glm(Wage,fit.step,K=10)$delta[1]
}

which.min(step.cv)
```
The minimum is degree of 5, but we can see the test error didn't change much  from degree 3 to 5. So I will choose degree of 3 as the optimal one

```{r,echo=F,warning=F}
plot(1:10,step.cv,type = "l",ylab = "cross-validation errors",xlab = "degree of freedom")
```

we can see that 8 cuts will give best fit 

```{r,echo=F,warning=F}
fit.step.op<-glm(wage~cut(age,8),data = Wage)

agelimits<-range(Wage$age)
age.range<-seq(from=agelimits[1],to=agelimits[2])
pred.step<-predict(fit.step.op,data.frame(age=age.range))

#Create plot for step function

par(mfrow=c(1,1))
with(Wage,plot(age,wage,xlim=agelimits,cex=.5,col="darkgrey"))
title("Degree-8 step function",outer = T)
lines(age.range, pred.step, col="red", lwd=2)
```


#7

investigate the wage to jobclass and race are categorical variables

```{r,echo=F,warning=F}
summary(Wage$race)
summary(Wage$jobclass)

par(mfrow=c(1,2))
with(Wage,plot(Wage$maritl,wage))
with(Wage,plot(Wage$jobclass,wage))

#Unable to fit splines on categorical variables.

library(gam)
#usig genearlized additive model
gam.0<-gam(wage~lo(age,span=0.7)+s(year,df=4),data = Wage)
gam.1<-gam(wage~lo(age,span=0.7)+s(year,df=4)+race,data = Wage)
gam.2<-gam(wage~lo(age,span=0.7)+s(year,df=4)+jobclass,data = Wage)
#Model 4
gam.3<-gam(wage~lo(age,span=0.7)+s(year,df=4)+race+jobclass,data = Wage)

anova(gam.0,gam.1,gam.2,gam.3,test="F")

#Based on the ANOVA results, M4 is perfered. using linear fit of race and jobclass. 
par(mfrow=c(2,2))
plot(gam.3,se=T,col="blue")
```

Based on the ANOVA results, M4 is perfered. using linear fit of race and jobclass.


#8

```{r,echo=F,warning=F}
pairs(Auto)
gam.1<-glm(mpg~displacement+weight+horsepower,data = Auto)
gam.2<-gam(mpg~poly(displacement,2)+lo(weight,span = 0.2)+s(horsepower,df=2),data = Auto)
gam.3<-gam(mpg~poly(displacement,3)+lo(weight,span = 0.2)+s(horsepower,df=2),data = Auto)
gam.4<-gam(mpg~poly(displacement,2)+lo(weight,span = 0.2)+s(horsepower,df=4),data = Auto)

anova(gam.1,gam.2,gam.3,gam.4,test = "F")

par(mfrow=c(1,3))
plot(gam.4,se=T,col="blue")
```

The test shows that there is no necessary for displacement go higher degree polynominal, but the horsepower is necessary for higher degree.there is nonlinear relationship between mpg to displacement, horsepower and weight


#9

##a
```{r,echo=F,warning=F}
library(MASS)
glm.boston<-glm(nox~poly(dis,3),data=Boston)
coef(summary(glm.boston))

dislimit<-range(Boston$dis)
par(mfrow=c(1,1))
dis.grid<-seq(from=dislimit[1],to=dislimit[2],by=0.1)
preds<-predict(glm.boston,newdata=list(dis=dis.grid),se=T)
se.bands<-cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)

par(mfrow=c(1,1),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
plot(Boston$dis,Boston$nox,xlab="weighted mean of distances to five Boston 
     employment centres",ylab="nitrogen oxides concentration")
title("Cubic Polynominal Regression")
lines(dis.grid,preds$fit,lwd=2,col="red")
matlines(dis.grid,se.bands,lwd=1,col="red")
```


##b

```{r,echo=F,warning=F}
for (i in 1:10){
  glm.boston<-lm(nox~poly(dis,i,raw =T),data=Boston)
  pred.fit<-predict(glm.boston,newdata=Boston)
}
rss.sum<-sum((Boston$dis-pred.fit)^2)
rss.sum
```

The sum of square of residual is `r rss.sum`

#c
```{r,echo=F,warning=F}
#perform cross validation for selecting the i
cross.error<-rep(0,10)
for (i in 1:10) {
  glm.boston<- glm(nox~poly(dis,i,raw = T),data=Boston)
  cross.error[i]<-cv.glm(Boston, glm.boston,K=10)$delta[1]
  pred<-predict(glm.boston,newdata=list(dis=dis.grid),se=T)
  pred.fit<-predict(glm.boston,newdata=Boston)
  rss.sum<-sum((Boston$dis-pred.fit)^2)
}
cross.error
which.min(cross.error)
#the polynominal degree of 3 gives the minimun cross-validation error
plot(1:10,cross.error,type = "l",xlab = "polynominal degree",ylab = "cross-validation error")
```

the polynominal degree of 3 gives the minimun cross-validation error
The cross-validation errors shows that 2 to 5 degree are almost same 7 degree and 9 degree tend to overfit
the data.

##d

```{r,echo=F,warning=F}
summary(Boston$dis)
#Becuase we need to use the degree of freedom is 4 and we need four interval, we need three cut points,
#I pick 1st, mean and 3rd quantitle to cut the interval. the points are 2, 4 and 6

spline.boston<- glm(nox~bs(dis,df=4,knot=c(2,4,6)),data=Boston)
summary(spline.boston)
dislimit<-range(Boston$dis)
dis.grid<-seq(from=dislimit[1],to=dislimit[2])

sp.pred<-predict(spline.boston,newdata = list(dis=dis.grid),se=T)

se.band<-cbind(sp.pred$fit-2*sp.pred$se.fit,sp.pred$fit+2*sp.pred$se.fit)
with(Boston,plot(dis,nox))
lines(dis.grid,sp.pred$fit,type="l",col="red")
matlines(dis.grid,se.band,type = "l",col = "red")
```

Becuase we need to use the degree of freedom is 4 and we need four interval, we need three cut points,
I pick 1st, mean and 3rd quantitle to cut the interval. the points are 2, 4 and 6

#e
```{r,echo=F,warning=F}
rss<-rep(0,15)
for (i in 1:15) {
dislimit<-range(Boston$dis)
dis.grid<-seq(from=dislimit[1],to=dislimit[2])

sr.fit<- glm(nox~ns(dis,df=i),data=Boston)

rss[i]<-sum(sr.fit$residuals^2)

}
plot(1:15,rss,type = "l",xlab = "Degree of Freedom",ylab = "Residual Sum of Square")
```

The RSS was decrease while the degree of freedom was increase, because we only use the training data, it tend to be overfit


##f

we add traing and test data set in selecting the results
```{r,echo=F,warning=F}
k=10
opt.df=11
set.seed(1)
folds<-sample(1:k,nrow(Boston),replace = T)
cv.errors=matrix(NA,k,opt.df-2,dimnames=list(NULL,paste(3:opt.df)))

for(j in 1:k){
  for(i in 3:opt.df){
    gam.fit<-lm(nox~bs(dis,df=i),data=Boston[folds!=j,])
    
    pred<-predict(gam.fit,Boston[folds==j,])
    cv.errors[j,i-2]=mean((Boston$nox[folds==j]-pred)^2)
  }
}
#calculate the mean errors over the column, if you want to calculate over row, then use 1
#col.sums <- apply(x, 2, sum)
#row.sums <- apply(x, 1, sum)

mean.cv.errors<-apply(cv.errors,2,mean)
mean.cv.errors
mean.cv.errors[which.min(mean.cv.errors)]
```

It seems like degree of 5 will give the optimal results


#10

##a

```{r,echo=F,warning=F}
library(ISLR)
library(leaps)
names(College)

train<-sample(dim(College)[1],1/2*dim(College)[1])
best.sub<-regsubsets(Outstate~.,data=College[train,],method = "forward",nvmax=17)

reg.summary<-summary(best.sub)

par(mfrow=c(1,3))
plot(reg.summary$cp,xlab="Number of Variables",ylab="AIC",type="l")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],col="red",cex=2,pch=20)

#It seems like 10 variable gives minimum AIC and 8 variables gives minimum BIC. 
plot(reg.summary$bic,xlab="Number of variables",ylab="BIC",type="l")
which.min(reg.summary$bic)
points(8,reg.summary$bic[8],col="red",cex=2,pch=20)

plot(reg.summary$adjr2,xlab="Number of variables",ylab="Adjusted RSQ",type="l")
which.max(reg.summary$adjr2)
points(13,reg.summary$adjr2[13],col="red",cex=2,pch=20)

#It seems like 8 variable will give be the optimal model, because according to AIC all the values are almost 
#same after 8 variables, there is no significant increase or decrease
coefi <- coef(best.sub, id = 8)
names(coefi)
#The six variables are "PrivateYes" ,"Accept" ,"F.Undergrad","Room.Board", "PhD", "perc.alumni"
# "Expend" "Grad.Rate"
```

It seems like 10 variable gives minimum AIC and 8 variables gives minimum BIC.
It seems like 8 variable will give be the optimal model, because according to AIC all the values are almost 
same after 8 variables, there is no significant increase or decrease
The six variables are "PrivateYes" ,"Accept" ,"F.Undergrad","Room.Board", "PhD", "perc.alumni", "Expend" "Grad.Rate"


##b

```{r,echo=F,warning=F}
library(gam)
par(mfrow=c(1,1))
#(College,plot(Grad.Rate,Outstate))
gam.fit<-gam(Outstate~Private+s(Accept,df=4)+lo(F.Undergrad,span = 0.4)+Room.Board+ns(PhD,df=4)+
               perc.alumni+s(Expend,df=4)+ns(Grad.Rate,df=4),data=College[train,])
summary(gam.fit)
#It seems like the 
par(mfrow=c(3,3))
plot.gam(gam.fit,se=T)
```

The variable perc.alumni and roomboard is has more linear relationship to the Outstate tution, and Accept,
undergrad, phd and grad rate are more non-linear


##c
```{r,echo=F,warning=F}
gam.pred<-predict(gam.fit, College[-train,])
err<-mean((College$Outstate[-train]-gam.pred)^2)
err

gam.tss<-mean((College$Outstate[-train]-mean(College$Outstate[-train]))^2)
test.rss<-1-err/gam.tss
test.rss

#linear model R square
glm.fit<-glm(Outstate~Private+Accept+F.Undergrad+Room.Board+PhD+
  perc.alumni+Expend+Grad.Rate,data=College[train,])
summary(glm.fit)
glm.pred<-predict(glm.fit, College[-train,])
glm.err<-mean((College$Outstate[-train]-glm.pred)^2)


glm.tss<-mean((College$Outstate[-train]-mean(College$Outstate[-train]))^2)
glm.test.rss<-1-glm.err/gam.tss
glm.test.rss
```

The test R square is 0.8255285 for GAM model, which is higher than linear model. 
The GAM model is improved compared to linear model

##d 

which variables, if any, is there evidence of a non-linear relationship with response

```{r,echo=F,warning=F}
summary(gam.fit)
```

The Accept, F.Undergrad,Expend shows a nonlinear relationship. The room.board and perc.alumin showed linear relationship

#11

##a

```{r,echo=F,warning=F}
set.seed(10)
x1<-rnorm(100)
x2<-rnorm(100)
eps<-rnorm(100,sd=0.1)
y=-2+1.5*x1+2*x2+eps
```

##b

```{r,echo=F,warning=F}
beta0<- rep(NA,1000)
beta1<-rep(NA,1000)
beta2<-rep(NA,1000)
beta1[1]=10
```

##c-e

```{r,echo=F,warning=F}
for (i in 1:1000){
  a=y-beta1[i]*x1
  beta2[i]=lm(a~x2)$coef[2]
  a=y-beta2[i]*x2
  lm.fit=lm(a~x1)
  if (i < 1000){
  beta1[i+1]=lm.fit$coef[2]
  }
  beta0[i]=lm.fit$coef[1]
}

par(mfrow=c(1,1))
plot(1:1000,beta0,type = "l",xlab = "iteration",ylab = "betas",
     ylim=c(-4,4),col="green")
lines(1:1000,beta1,col="red")
lines(1:1000,beta2,col="blue")
legend("right",c("beta0","beta1","beta2"),lty = 1,
       col=c("green","red","blue"))
```

The coefficient decrease and obtain a steady value quickly

##f
```{r,echo=F,warning=F}
plot(1:1000,beta0,type = "l",xlab = "iteration",ylab = "betas",
     ylim=c(-4,4),col="green")
lines(1:1000,beta1,col="red")
lines(1:1000,beta2,col="blue")
lm.fit<-lm(y~x1+x2)
abline(h=lm.fit$coef[1],lty="dashed")
abline(h=lm.fit$coef[2],lty="dashed")
abline(h=lm.fit$coef[3],lty="dashed")
legend("right",c("beta0","beta1","beta2","multiple regression"),lty = c(1,1,1,2),cex=0.5,
       col=c("green","red","blue","black"))
```

From the plot, we can see that the dot line which is multiple regression coefficients, is exactly
the same with the estimate coefficients

##g

When the y and x relationship is linear, the iteration is not as much as possible


#12
```{r,echo=F,warning=F}
set.seed(11)
p=100
n=100
x=matrix(ncol = p,nrow = n)
coefi<-rep(NA,p)
for (i in 1:p){
  x[,i]=rnorm(n)
  coefi[i]=rnorm(1)*100

}
y=x%*%coefi+rnorm(n)

beta=rep(0,p)
m_i=1000
error<-rep(NA,m_i+1)
e=2
error[1]=Inf
error[2]=sum((y-x%*%beta)^2)
threshold<-1e-04
while (e < m_i && error[e - 1] - error[e] > threshold){
  for (i in 1:p){
    a=y-x%*%beta+beta[i]*x[,i]
    beta[i]=lm(a~x[,i])$coef[2]
  }
  e=e+1
  error[e]=sum((y-x%*%beta)^2)
  print(c(e-2,error[e-1],error[e]))
}
plot(1:20,error[3:22],type = "l")
points(10,error[12],col="red")

```

It seems like after 10 iteration, there is not too much change, although 78 iteration will gives us minimum value,








