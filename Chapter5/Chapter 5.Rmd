---
title: "Chapter 5"
author: "Wenhui Zeng"
date: "February 9, 2017"
output: pdf_document
---

##1
$$Var(x+y)=Var(x)+Var(y)+2Cov(x,y)$$


$$Var(cx)=C^2Var(x)$$
$$Cov(cx,y)=Cov(x,cy)=cCov(x,y)$$
minimizing two asset finicial portfolio:
$$Var(x\alpha+(x-\alpha)y)=Var(x\alpha)+Var((1-\alpha)y)+2Cov(\alpha x,(1-\alpha)y)=\alpha^2Var(x)+(1-\alpha)^2Var(y)+2\alpha(1-\alpha)Cov(x,y)$$
$$\sigma_x^2\alpha^2+\sigma_y^2y^2(1-\alpha)^2+2\sigma_{xy}(-\alpha^2+\alpha)$$
Take first derivative to find critical point
$$ \frac {d} {d\alpha} f(\alpha)=0$$
$$2\sigma_x^2\alpha-2\sigma_y^2(1-\alpha)+2\sigma_{xy}(1-2\alpha)$$
$$\alpha=\frac{\sigma_y^2-\sigma_{xy}}{\sigma_x^2+\sigma_y^2-2\sigma_xy}$$

##2

###a

$$1-\frac{1}{n}$$

###b

$$1-\frac{1}{n}$$

###c

In bootstrap, we sample with replacement so each observation in the boostrap sample has the same  independent chance of equaling the jth observation.
Applying the product for a total of n observation gives us 
$$p(out)=(1-\frac{1}{n})^n$$

###d
n=5
P(x)=1-P(out)=67.2%


###e

n=100
P(in)=1-p(100)=63.4%

###f

n=10000
p(x)=1-p(10000)=63.2%

###g
```{r,echo=F,warning=F}
a=function(n){
  return (1-(1-1/n)^n)
}
x=1:100000
plot(x,a(x))
```

###h 
```{r,echo=F,warning=F}
store=rep(NA,10000)
for (i in 1:10000){
  store[i]=sum(sample(1:100,rep=TRUE)==4)>0
}
a<-mean(store)
```

bootstrap sampe size is 100, contains the jth observation. j=4.the mean is `r a`

##3

###a

K fould cross-validation is implemented by taking the set of n observations and round only splitting into k non-overlaping groups. Each of these groups acts as a validation set and the reminding as a training set.

###b

The validation set approach is conceptially simple and easily implemented as you are simply partitioning the existing training data into two sets.
However, there are two drawbacks:
(1) The estimate of the test error rate can be highly variable depending on which observations are included in the training and validation set.
(2) The validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set

##ii

LOOCV is a special case of k-fold cross-validation with k=1. Thus LOOCV is the most computationaly intense method since the model must be fit n times. Also, LOOCV has higher variance, but lower bias than k-fold CV

##4

If we suppose using some statistical learning method to make a prediction for the response Y for a particular value of the prediction X. We might estimate the standard deviation of our prediction by suing the bootstrap approach. The boostrap approach works by repeatedly sampling obsrevation(with replacement from the original data set B times). For a large value of B, each time fitting a new model and subsequently obtaining the RMSE of the estimate for all B models.


##5

###a

```{r,echo=F,warning=F}
library(ISLR)
names(Default)
glm.fit<-glm(default~income+balance,family = binomial,data=Default)
summary(glm.fit)
```

split the data into training and validation data set using the sample() function

```{r,echo=F,warning=F}
default=function() {
train = sample(dim(Default)[1], dim(Default)[1]/2)
#fit a multiple losgistic regressiong using only the train data
glm.fit.train<-glm(default~income+balance,family = binomial,data=Default, subset=train)
glm.pred<-rep("No",dim(Default)[1]/2)
glm.prob<-predict(glm.fit.train,Default[-train,],type="response")
glm.pred[glm.prob>0.5]="Yes"
table(glm.pred,Default[-train,]$default)
mean(glm.pred!=Default[-train,]$default)
}

```

repreat the process in b with three times, because it has sample function, just run three times
default()

###d

```{r,echo=F,warning=F}
  train = sample(dim(Default)[1], dim(Default)[1]/2)
  
  glm.fit.train=glm(default~income+balance+student,family = binomial,data=Default, subset=train)
  #
  glm.pred=rep("No",dim(Default)[1]/2)
  glm.prob=predict(glm.fit.train,Default[-train,],type="response")
  glm.pred[glm.prob>0.5]="Yes"
  table(glm.pred,Default[-train,]$default)
  mean(glm.pred!=Default[-train,]$default)
```

fit a multiple losgistic regressiong using only the train data.test error rate, with student dummy variable. Using the validation set approach, it doesn't appear adding the student dummy variable leads to a reduction in the test error rate.

##6

###a
```{r,echo=F,warning=F}
library(ISLR)
glm.fit=glm(default~income+balance,data=Default,family=binomial)
summary(glm.fit)
coef(glm.fit)
```

the standard error of the intercept, income, balance are 4.348e-01,4.985e-06,2.274e-04 

###b

```{r,echo=F,warning=F}
boot.fn=function(data,index){
   return(coef(glm(default~income+balance,data=data,family = binomial,subset=index)))
}
summary(Default)
```

###c

```{r,echo=F,warning=F}
library(boot)
boot(Default,boot.fn,50)
#the boot function is the same as 
set.seed(1)
dim(Default[1])
boot(Default,boot.fn,50)
boot.fn(Default,sample(dim(Default)[1]/2,50))
```

###d

The bootstap method, the standard error for the intercept, income, balance are 4.202402e-01,4.542214e-06,2.282819e-04.
The standard deviation was more small than the genearl linear model. 

##7

###a
```{r,echo=F,warning=F}
glm.fit<-glm(Direction~Lag1+Lag2,data=Weekly,family=binomial)
summary(glm.fit)
```


###b

fit the logistic regression using all but the first obs
```{r,echo=F,warning=F}
glm.fit.1<-glm(Direction~Lag1+Lag2,data=Weekly[-1,],family = binomial)
```

###c

```{r,echo=F,warning=F}
predict(glm.fit, Weekly[1, ], type = "response") > 0.5
predict.glm(glm.fit, Weekly[1, ], type = "response") > 0.5
```

it was predicted as going up, however the rist one is going down. It was classfied wrong

###d

```{r,echo=F,warning=F}
count = rep(0, dim(Weekly)[1])
for (i in 1:(dim(Weekly)[1])){
  glm.fit.loop=glm(Direction~Lag1+Lag2,data=Weekly[-i,],family = binomial)
  up=predict.glm(glm.fit.loop,Weekly[i,],type="response")>0.5
  true.up=Weekly[i,]$Direction=="Up"
  if (up!=true.up) 
    count[i]=1
  
}

sum(count)
```

490 errors.

###e

```{r,echo=F,warning=F}
mean(count)
```

LOOCV estimates a test error rate of 45%.


##8

###a

```{r,echo=F,warning=F}
set.seed(1)
y=rnorm(100)
x=rnorm(100)
y=x-2*x^2+rnorm(100)
```

###b

```{r,echo=F,warning=F}
plot(x,y)
```

Quadratic plot. XX from about -2 to 2. YY from about -8 to 2.

###c

###i

```{r,echo=F,warning=F}
data<-data.frame(x,y)
glm.fit<-glm(y~x,data=data)
library(boot)
set.seed(1)
cv.err<-cv.glm(data,glm.fit)
a<-cv.err$delta
```

LOOCV error is `r a`. cv.glm This function calculates the estimated K-fold cross-validation prediction error for generalized. linear models.(have to be glm model otherwise it may fail) It is part of the boot library

####ii

```{r,echo=F,warning=F}
glm.fit2=glm(y~poly(x,2),data=data)
set.seed(1)
cv.err1<-cv.glm(data,glm.fit2)
a<-cv.err1$delta
```

LOOCV error is `r a`

####iii

```{r,echo=F,warning=F}
glm.fit3=glm(y~poly(x,3),data=data)
set.seed(1)
cv.err2=cv.glm(data,glm.fit3)
a<-cv.err2$delta
```

LOOCV error is `r a`

####iv

```{r,echo=F,warning=F}
glm.fit4=glm(y~x+poly(x,4))
set.seed(1)
cv.err3=cv.glm(data,glm.fit4)
a<-cv.err3$delta
```

LOOCV error is `r a`


###d

####i
```{r,echo=F,warning=F}
set.seed(3)
cv.err=cv.glm(data,glm.fit)
a<-cv.err$delta
```

LOOCV error is `r a`

####ii

```{r,echo=F,warning=F}
glm.fit2=glm(y~poly(x,2),data=data)
set.seed(3)
cv.err1=cv.glm(data,glm.fit2)
a<-cv.err1$delta
```

LOOCV error is `r a`

####iii

```{r,echo=F,warning=F}
glm.fit3=glm(y~poly(x,3),data=data)
set.seed(3)
cv.err2=cv.glm(data,glm.fit3)
a<-cv.err2$delta
```

LOOCV error is `r a`

####iv

```{r,echo=F,warning=F}
glm.fit4=glm(y~x+poly(x,4))
set.seed(3)
cv.err3=cv.glm(data,glm.fit4)
a<-cv.err3$delta
```

LOOCV error is `r a`. Exact same, because LOOCV will be the same since it evaluates n folds of a single observation

###e

The quadratic polynomial had the lowest LOOCV test error rate.so the quadratic will give the best fit 
This was expected because it matches the true form .

###f
```{r,echo=F,warning=F}
summary(glm.fit4)
```

p-values show statistical significance of linear and quadratic terms, which agrees with the CV results.



#9

##a
```{r,echo=F,warning=F}
library(MASS)
summary(Boston)
a<-mean(Boston$medv)
a
```

The mean is `r a`

##b
```{r,echo=F,warning=F}
std.err<-sd(Boston$medv)/sqrt(length(Boston$medv))
```

The standard error is `r std.err`

##c 
```{r,echo=F,warning=F}
#estimate the standard using bootstrap
boot.fn=function(data,index){
  return(mean(data[index]))
}

library(boot)
medv.err<-boot(Boston$medv,boot.fn,1000)
```

The standard error using bootstrap is similar to answer from estimate from t estimate


#d
```{r,echo=F,warning=F}
t.test(Boston$medv)
#confidence interval (21.72953 23.33608)
cv.up=22.53281+2*0.4119
cv.low=22.53281-2*0.4119
```

The 95% confidence interval is (`r cv.up`,`r cv.low`)

Bootstrap estimate only 0.02 away for t.test estimate 


#e
```{r,echo=F,warning=F}
median<-median(Boston$medv)
```

The median is `r median`

#f
```{r,echo=F,warning=F}
boot.fn=function(data,index){
  return(median(data[index]))
}

library(boot)
med<-boot(Boston$medv,boot.fn,1000)
```

Median of `r median` with SE of 0.38 is Small standard error relative to median value.

#g
```{r,echo=F,warning=F}
medv.tenth <- quantile(Boston$medv, c(0.1))
medv.tenth1 <- quantile(Boston$medv, probs=0.1)
```

quantile(x, probs = n) x is the numeric vector whose sample quantiles are wanted, or an object of 
a class for which a method has been defined
numeric vector of probabilities with values in [0,1]


#h
```{r,echo=F,warning=F}
set.seed(1)
boot.fn=function(data,index){
  return(quantile(data[index],0.1))
}

a<-boot(Boston$medv, boot.fn, 1000)
```

Tenth-percentile of 12.75 with SE of 0.511. Small standard error relative to tenth-percentile value.




















































