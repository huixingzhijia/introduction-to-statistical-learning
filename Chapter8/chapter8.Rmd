---
title: "Chapter 8"
author: "Wenhui Zeng"
date: "February 12, 2017"
output: pdf_document
---
#1
```{r,echo=F,warning=F}
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(0, 50), ylim = c(0, 50), xlab = "X", ylab = "Y")
# t1: x = 40; (40, 0) (40, 100)
lines(x = c(20, 20), y = c(0, 50))
text(x = 20, y = 58, labels = c("a1"), col = "red")
# t2: y = 75; (0, 75) (40, 75)
lines(x = c(0, 20), y = c(35, 35))
text(x = -8, y = 35, labels = c("a2"), col = "red")
# t3: x = 75; (75,0) (75, 100)
lines(x = c(10,10 ), y = c(0, 35))
text(x = 10, y = 38, labels = c("a3"), col = "red")
# t4: x = 20; (20,0) (20, 75)
lines(x = c(20,50), y = c(45, 45))
text(x = 55, y = 45, labels = c("a4"), col = "red")
# t5: y=25; (75,25) (100,25)
lines(x = c(37, 37), y = c(0, 45))
text(x = 37, y = 47, labels = c("a5"), col = "red")

text(x = (0+10)/2, y = 17, labels = c("R1"))
text(x = 15, y = 17, labels = c("R2"))
text(x = 10, y = 43, labels = c("R3"))
text(x = 30, y = 30, labels = c("R4"))
text(x = 43, y = 35, labels = c("R5"))
text(x = 40, y = 48, labels = c("R6"))
```


#2

Using one-depth will consist of a split on a single variable.. By inductio, the residuals of that first fit will result in a second stump fit to another distinct.

$$f(x)=\sum_{j=1}^{p}f_j(x_j)$$
$$\hat{f}(x)=0, r_i=y_i$$

$$\hat{f^1}(x)=\beta_1I(x_1<t_1)+\beta_0$$
$$\hat{f}(x)=\lambda\hat{f^1}(x)$$
$$r_i=y_i-\lambda\hat{f^1}(x)$$
To maxmize the fit to the residuals, another distinct stump must be fit in the next and subsequent iterations will each fit $x_j$ distinct stumps.
For the $jth$ observation $b=j$

$$\hat{f^j}(x)=\beta_1I(x_1<t_1)+\beta_0$$

$$\hat{fs}(x)=\lambda\hat{f}^1(x_1)+\cdots+\hat{f}^j(x_j)+\cdots+\hat{f}^{p-1}(x_{p-1})+\hat{f}^p(x_p)$$

Since each iteration's fit is a distinct variable, there are only *p* fits based on it.

#3
$$p=\hat{p}_{mk}$$, There are two classes, so there is 2 in front of the equation



```{r,echo=F,warning=F}
#because there are two classes, 
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), col = c("red", "green", "black"))
```
 
#5

Using the major approch, like flip the coin, we define if the $P>0.5$ so the event is happen. 
```{r,echo=F,warning=F}
a<-c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
#the number of the probability that greater than 0.5
g<-sum(a>0.5)
l<-sum(a<0.5)
```

We can see that the number of probability greater than 0.5 `r g` is greater than the number of probability less than 0.5 `r l`. So it should be red

The second approach is using the mean or average

```{r,echo=F,warning=F}
p<-mean(a)
```

The average probability is `r p`, which is less than 0.5, so we say it was green. 









