---
title: "Chapter 8"
author: "Wenhui Zeng"
date: "February 12, 2017"
output: pdf_document
---
#1
```{r,echo=F,warning=F}
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(0, 50), ylim = c(0, 50), xlab = "X", ylab = "Y")
# t1: x = 40; (40, 0) (40, 100)
lines(x = c(20, 20), y = c(0, 50))
text(x = 20, y = 58, labels = c("a1"), col = "red")
# t2: y = 75; (0, 75) (40, 75)
lines(x = c(0, 20), y = c(35, 35))
text(x = -8, y = 35, labels = c("a2"), col = "red")
# t3: x = 75; (75,0) (75, 100)
lines(x = c(10,10 ), y = c(0, 35))
text(x = 10, y = 38, labels = c("a3"), col = "red")
# t4: x = 20; (20,0) (20, 75)
lines(x = c(20,50), y = c(45, 45))
text(x = 55, y = 45, labels = c("a4"), col = "red")
# t5: y=25; (75,25) (100,25)
lines(x = c(37, 37), y = c(0, 45))
text(x = 37, y = 47, labels = c("a5"), col = "red")

text(x = (0+10)/2, y = 17, labels = c("R1"))
text(x = 15, y = 17, labels = c("R2"))
text(x = 10, y = 43, labels = c("R3"))
text(x = 30, y = 30, labels = c("R4"))
text(x = 43, y = 35, labels = c("R5"))
text(x = 40, y = 48, labels = c("R6"))
```


#2

Using one-depth will consist of a split on a single variable.. By induction, the residuals of that first fit will result in a second stump fit to another distinct model.

$$f(x)=\sum_{j=1}^{p}f_j(x_j)$$
$$\hat{f}(x)=0, r_i=y_i$$

$$\hat{f^1}(x)=\beta_1I(x_1<t_1)+\beta_0$$
$$\hat{f}(x)=\lambda\hat{f^1}(x)$$
$$r_i=y_i-\lambda\hat{f^1}(x)$$
To maxmize the fit to the residuals, another distinct stump must be fit in the next and subsequent iterations will each fit $x_j$ distinct stumps.
For the $jth$ observation $b=j$

$$\hat{f^j}(x)=\beta_1I(x_1<t_1)+\beta_0$$

$$\hat{fs}(x)=\lambda\hat{f}^1(x_1)+\cdots+\hat{f}^j(x_j)+\cdots+\hat{f}^{p-1}(x_{p-1})+\hat{f}^p(x_p)$$

Since each iteration's fit is a distinct variable, there are only *p* fits based on it.

#3
$$p=\hat{p}_{mk}$$, There are two classes, so there is 2 in front of the equation



```{r,echo=F,warning=F}
#because there are two classes, 
p = seq(0, 1, 0.01)
gini = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
class.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini, entropy, class.err), col = c("red", "green", "black"))
```
 
#5

Using the major approch, like flip the coin, we define if the $P>0.5$ so the event is happen. 
```{r,echo=F,warning=F}
a<-c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
#the number of the probability that greater than 0.5
g<-sum(a>0.5)
l<-sum(a<0.5)
```

We can see that the number of probability greater than 0.5 `r g` is greater than the number of probability less than 0.5 `r l`. So it should be red

The second approach is using the mean or average

```{r,echo=F,warning=F}
p<-mean(a)
```

The average probability is `r p`, which is less than 0.5, so we say it was green. 

##6

First we do recursive binary splitting on the data. This is a top-down approach where recursively and greedily we find the best single partitioning of the data such that the reduction of RSS is the greatest. This process is applied to each of the split parts seperately until some minimal number of observations is present on each of the leaves.

Then we apply cost complexity pruning of this larger tree formed in step 1 to obtain a sequence of best subtrees as a function of a parameter, $\alpha$. Each value of $\alpha$ corresponds to a different subtree which minimizes the equation $$\sum_{m=i}^{|T|}\sum_{i:x_i\in R_m}(y_i - \hat y_{R_m})^2 + \alpha |T|.$$ Here $|T|$ is the number of terminal nodes on the tree. When $\alpha=0$ we have the original tree, and as $\alpha$ increases we get a more pruned version of the tree.

Next, using K-fold CV, choose $\alpha$. For each fold, repeat steps 1 and 2, and then evaluate the MSE as a function of $\alpha$ on the held out fold. Chose an $\alpha$ that minimizes the average error.

Given the size chosen in previous step, return the tree calculated using the formula laid out in step 2 on the entire dataset with that chosen value of best=size(how many node do we have).

 
#7

```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
library(tree)
library(ISLR)
library(randomForest)
library(MASS)
library(gbm)
library(xtable)

set.seed(1)
train<-sample(1:nrow(Boston),nrow(Boston)/2)
x.train<-Boston[train,-14]
x.test<-Boston[-train,-14]
y.test<-Boston$medv[-train]
y.train<-Boston$medv[train]



rf.boston.25.1<-randomForest(x=x.train,y=y.train,xtest=x.test,ytest = y.test,ntree=25,mtry=(ncol(Boston)-1)/2)
rf.boston.25.2<-randomForest(x=x.train,y=y.train,xtest=x.test,ytest = y.test,ntree=25,mtry=ncol(Boston)-1)
rf.boston.25.3<-randomForest(x=x.train,y=y.train,xtest=x.test,ytest = y.test,ntree=25,mtry=sqrt(ncol(Boston-1)))
rf.boston.500.1<-randomForest(x=x.train,y=y.train,xtest=x.test,ytest = y.test,ntree=500,mtry=(ncol(Boston)-1)/2)
rf.boston.500.2<-randomForest(x=x.train,y=y.train,xtest=x.test,ytest = y.test,ntree=500,mtry=ncol(Boston)-1)
rf.boston.500.3<-randomForest(x=x.train,y=y.train,xtest=x.test,ytest = y.test,ntree=500,mtry=sqrt(ncol(Boston-1)))


plot(1:25, rf.boston.25.1$test$mse, col = "green", type = "l", xlab = "Number of Trees", ylab = "Test MSE")
lines(1:25, rf.boston.25.2$test$mse, col = "red", type = "l")
lines(1:25, rf.boston.25.3$test$mse, col = "blue", type = "l")
legend("topright", c("m = p", "m = p/2", "m = sqrt(p)"), col = c("green", "red", "blue"), lty = 1)

par(mfrow=c(1,1))
plot(1:500, rf.boston.500.1$test$mse, col = "green", type = "l", xlab = "Number of Trees", ylab = "Test MSE")
lines(1:500, rf.boston.500.2$test$mse, col = "red", type = "l")
lines(1:500, rf.boston.500.3$test$mse, col = "blue", type = "l")
legend("topright", c("m = p", "m = p/2", "m = sqrt(p)"), col = c("green", "red", "blue"), lty = 1,cex=1, pch=1, pt.cex = 1)

#The random forest has two forms, 1 is using the randomForest(formular,data=Null,subset=)
#The other one is the following one, the following one is easy to calculate the test error and calculate the result
```

For both plots, the test MSE was decreasing as the number of trees increased. For the growing trees upto25, 
There is not too much difference between using different number of variables.
However, when we growing trees upto 500, we can see that the bagging gives the smallest test MSE. after around 140 trees, the bagging and random forest are almost same.

#8

##a-b

```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
set.seed(1)
train<-sample(1:nrow(Carseats),nrow(Carseats)/2)
x.train<-Carseats[train,]
x.test<-Carseats[-train,]
y.train<-Carseats$Sales[train]
y.test<-Carseats$Sales[-train]
tree.car<-tree(Sales~.,data = Carseats,subset = train)
plot(tree.car)
text(tree.car,pretty=0)
```

Describe the trees:The unit sales was high for the location,where the
average age is greater than 62,the quality of the shelving is good and the
the charges for car seats was less than 113. The average sale price was $8,600 dollars

##c

```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
yhat<-predict(tree.car,newdata=x.test)
boston.test=Boston$medv[-train]
plot(yhat,y.test)
test.mse<-mean((yhat-y.test)^2)
```

The test MSE is `r test.mse`.

##c

```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
#cross-validation select the alpha and terminal node
#The default is  FUN = prune.tree
set.seed(1)
cv.car<-cv.tree(tree.car,FUN = prune.tree)
par(mfrow=c(1,2))
plot(cv.car$size,cv.car$dev,type="b")
plot(cv.car$k,cv.car$dev,type="b")
#The lowest MSE has 8 terminal nodes

prune.car<-prune.tree(tree.car,best=8)
plot(prune.car)
text(prune.car,pretty=0)

yhat<-predict(prune.car,newdata=x.test)
plot(yhat,y.test)
test.mse<-mean((yhat-y.test)^2)
```

The test error is `r test.mse`. The test error was increased. There is no improvement of the test error.

##d
```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
set.seed(1)
#mtry=13 indicates that all 13 predictors should be considered for each split of the tree. that bagging should be done
p<-ncol(Carseats)-1
bag.car<-randomForest(Sales~.,data=Carseats,subset=train,mtry=p,importance=T)
importance(bag.car)

#how well does this bagged model perform on the test set
yhat.bag<-predict(bag.car,newdata=x.test)
plot(yhat.bag,y.test)
abline(0,1)
test.mse<-mean((yhat.bag-y.test)^2)
```

The test MSE was `r test.mse` and we can see that the most important three variables were price, shelveloc, and age The test error was decrease

##d
```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
rf.car<-randomForest(Sales~.,data=Carseats,subset=train,mtry=sqrt(p),importance=T)
importance(rf.car)
#how well does this bagged model perform on the test set
yhat.bag<-predict(rf.car,newdata=x.test)
plot(yhat.bag,y.test)
abline(0,1)
test.mse<-mean((yhat.bag-y.test)^2)
```

The test MSE was `r test.mse` and we again see that the most important three variables were price, 
shelveloc, and age The test error was increased from bagging, but smaller than single tree. 

#9

##a-b

```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
train<-sample(1:nrow(OJ),800)
#names(OJ)
x.train<-OJ[train,]
x.test<-OJ[-train,]
y.test<-OJ$Purchase[-train]
y.train<-OJ$Purchase[train]
##b
oj.tree<-tree(Purchase~.,data=x.train)
summary(oj.tree)
```

The training error is $15.625$%, the number of terminal nodes is 7.

##c
```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
oj.tree
```

Let's pick terminal node labeled 27. The splitting variable at this node is ListpriceDiff, $\tt{ListpriceDiff}$. The splitting value of this node is $0.135$. There are $157$ points in the subtree below this node.
The deviance for all points contained in region below this node is $134.30$. A * in the line denotes that 
this is in fact a terminal node. The prediction at this node is $\tt{Sales}$ = $\tt{CH}$. About $84.713$% 
points in this node have $\tt{CH}$ as value of $\tt{Sales}$. Remaining $15.287$% points have $\tt{MM}$ as 
value of $\tt{Sales}$.

##d
```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
par(mfrow=c(1,1))
plot(oj.tree)
text(oj.tree,pretty=0)
```

$\tt{LoyalCH}$ is the most important variable of the tree, in fact top 3 nodes contain $\tt{LoyalCH}$. 
If $\tt{LoyalCH} < 0.27$, the tree predicts $\tt{MM}$. If $\tt{LoyalCH} > 0.76$, the tree predicts
$\tt{CH}$. For intermediate values of $\tt{LoyalCH}$, the decision also depends on the value of 
$\tt{PriceDiff}$.

#e
```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
tree.pred<-predict(oj.tree,x.test,type="class")
table(tree.pred, y.test)
test.error<-mean(tree.pred!=y.test)
```

The test MSE was `r test.error`

#f-h
```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
cv.oj<-cv.tree(oj.tree,FUN = prune.tree)
par(mfrow=c(1,2))
plot(cv.oj$size,cv.oj$dev,type="b")
plot(cv.oj$k,cv.oj$dev,type="b")
```

it seems like 5 node gives a good prediction. 

#i-j
```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
prune.oj<-prune.tree(oj.tree,best=5)
summary(prune.oj)
```

The training error is 130/800, there is 5 terminal node. The training error was increased. 

#k
```{r,echo=F,warning=F,error=F,fig.width=16, fig.height=6,message=F}
tree.pred<-predict(prune.oj,x.test,type="class")
table(tree.pred, y.test)
test.error<-mean(tree.pred!=y.test)
```

The test MSE was `r test.error`. It was decrease.












